"""
æ•¸æ“šè™•ç†æ§åˆ¶å™¨æ¨¡å¡Šï¼Œè² è²¬åŸ·è¡Œæ•¸æ“šè™•ç†å’Œåœ–åƒç”Ÿæˆä»»å‹™
"""
import os
import json
import uuid
import pandas as pd
import threading
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Callable
import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from PySide6.QtCore import QObject, Signal, QMetaObject, Qt, QTimer
import shutil
from queue import Queue, Empty
from datetime import timedelta

from ..utils import (
    get_logger, config, ensure_directory, 
    load_csv, find_header_row, save_df_to_csv,
    convert_to_binary, flip_data, apply_mask,
    calculate_loss_points, plot_basemap, 
    plot_lossmap, plot_fpy_map, plot_fpy_bar,
    check_csv_alignment, remove_header_and_rename,
    AOI_FILENAME_PATTERN, PROCESSED_FILENAME_PATTERN,
    extract_component_from_filename
)
from ..models import (
    db_manager, ComponentInfo, ProcessingTask
)

logger = get_logger("data_processor")


class ProcessingTask:
    """è™•ç†ä»»å‹™ï¼Œç”¨æ–¼è¿½è¹¤é•·æ™‚é–“é‹è¡Œçš„è™•ç†æ“ä½œ"""
    
    def __init__(self, task_id: str, task_type: str, product_id: str, lot_id: str, station: str = None, component_id: str = None):
        self.task_id = task_id
        self.task_type = task_type
        self.product_id = product_id
        self.lot_id = lot_id
        self.station = station
        self.component_id = component_id
        self.status = "pending"  # pending, running, completed, failed
        self.message = ""
        self.created_at = datetime.datetime.now()
        self.completed_at = None
    
    def start(self):
        """æ¨™è¨˜ä»»å‹™ç‚ºé‹è¡Œä¸­"""
        self.status = "running"
    
    def complete(self, message: str = ""):
        """æ¨™è¨˜ä»»å‹™ç‚ºå·²å®Œæˆ"""
        self.status = "completed"
        self.message = message
        self.completed_at = datetime.datetime.now()
    
    def fail(self, message: str = ""):
        """æ¨™è¨˜ä»»å‹™ç‚ºå¤±æ•—"""
        self.status = "failed"
        self.message = message
        self.completed_at = datetime.datetime.now()
    
    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸"""
        return {
            "task_id": self.task_id,
            "task_type": self.task_type,
            "product_id": self.product_id,
            "lot_id": self.lot_id,
            "station": self.station,
            "component_id": self.component_id,
            "status": self.status,
            "message": self.message,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "completed_at": self.completed_at.isoformat() if self.completed_at else None
        }


# ä»»å‹™å›èª¿ä¿¡è™Ÿå™¨é¡
class TaskSignaler(QObject):
    """ç”¨æ–¼åœ¨ç·šç¨‹é–“å®‰å…¨å‚³éä¿¡è™Ÿçš„é¡"""
    task_completed = Signal(str, bool, str)


class DelayedMoveManager(QObject):
    """å»¶é²ç§»å‹•ç®¡ç†å™¨ï¼Œè™•ç†å¤§é‡æª”æ¡ˆçš„å»¶é²ç§»å‹•"""
    
    def __init__(self):
        super().__init__()
        self.move_queue = Queue()
        self.scheduler = QTimer()
        self.scheduler.setSingleShot(True)  # è¨­ç½®ç‚ºå–®æ¬¡è§¸ç™¼
        self.scheduler.timeout.connect(self.process_delayed_moves)
        self.is_running = False
        
        # é‡è©¦æ©Ÿåˆ¶ç›¸é—œ
        self.failed_components = {}  # è¨˜éŒ„å¤±æ•—çš„çµ„ä»¶
        self.retry_enabled = config.get("auto_move.retry_mechanism.enabled", True)
        self.retry_on_partial_failure = config.get("auto_move.retry_mechanism.retry_on_partial_failure", True)
        
        logger.info("å»¶é²ç§»å‹•ç®¡ç†å™¨å·²åˆå§‹åŒ–")
        
    def add_to_delayed_queue(self, component_id: str, lot_id: str, station: str, 
                            source_product: str, target_product: str):
        """æ·»åŠ åˆ°å»¶é²ç§»å‹•éšŠåˆ—"""
        # æª¢æŸ¥æ˜¯å¦ç‚ºé‡è©¦ä»»å‹™
        if hasattr(self, 'failed_components') and component_id in self.failed_components:
            failure_info = self.failed_components[component_id]
            if failure_info['retry_count'] < config.get("auto_move.retry_mechanism.max_retry_count", 3):
                logger.info(f"çµ„ä»¶ {component_id} ç‚ºé‡è©¦ä»»å‹™ï¼Œé‡è©¦æ¬¡æ•¸: {failure_info['retry_count']}")
            else:
                logger.warning(f"çµ„ä»¶ {component_id} å·²è¶…éæœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œè·³é")
                return
        
        self.move_queue.put({
            'component_id': component_id,
            'lot_id': lot_id,
            'station': station,
            'source_product': source_product,
            'target_product': target_product,
            'timestamp': datetime.datetime.now()
        })
        logger.info(f"å·²æ·»åŠ åˆ°å»¶é²ç§»å‹•éšŠåˆ—: {component_id}")
    
    def record_component_failure(self, component_id: str, lot_id: str, station: str, 
                                source_product: str, target_product: str, failure_reason: str):
        """è¨˜éŒ„çµ„ä»¶ç§»å‹•å¤±æ•—"""
        if not self.retry_enabled:
            return
            
        if component_id not in self.failed_components:
            self.failed_components[component_id] = {
                'lot_id': lot_id,
                'station': station,
                'source_product': source_product,
                'target_product': target_product,
                'failure_reason': failure_reason,
                'retry_count': 0,
                'first_failure_time': datetime.datetime.now(),
                'last_failure_time': datetime.datetime.now()
            }
        else:
            self.failed_components[component_id]['retry_count'] += 1
            self.failed_components[component_id]['last_failure_time'] = datetime.datetime.now()
            self.failed_components[component_id]['failure_reason'] = failure_reason
            
        logger.warning(f"è¨˜éŒ„çµ„ä»¶ {component_id} ç§»å‹•å¤±æ•—: {failure_reason}")
    
    def get_failed_components_summary(self) -> dict:
        """ç²å–å¤±æ•—çµ„ä»¶æ‘˜è¦"""
        summary = {
            'total_failed': len(self.failed_components),
            'components': {}
        }
        
        for component_id, info in self.failed_components.items():
            summary['components'][component_id] = {
                'retry_count': info['retry_count'],
                'failure_reason': info['failure_reason'],
                'last_failure_time': info['last_failure_time'].isoformat()
            }
            
        return summary
    
    def cleanup_expired_failures(self):
        """æ¸…ç†éæœŸçš„å¤±æ•—è¨˜éŒ„ï¼ˆ24å°æ™‚å¾Œè‡ªå‹•æ¸…ç†ï¼‰"""
        current_time = datetime.datetime.now()
        expired_components = []
        
        for component_id, info in self.failed_components.items():
            time_diff = current_time - info['first_failure_time']
            if time_diff.total_seconds() > 24 * 3600:  # 24å°æ™‚
                expired_components.append(component_id)
                
        for component_id in expired_components:
            del self.failed_components[component_id]
            logger.info(f"æ¸…ç†éæœŸçš„å¤±æ•—è¨˜éŒ„: {component_id}")
    
    def reset_failure_record(self, component_id: str):
        """é‡ç½®çµ„ä»¶çš„å¤±æ•—è¨˜éŒ„"""
        if component_id in self.failed_components:
            del self.failed_components[component_id]
            logger.info(f"é‡ç½®çµ„ä»¶ {component_id} çš„å¤±æ•—è¨˜éŒ„")
    
    def get_failure_statistics(self) -> dict:
        """ç²å–å¤±æ•—çµ±è¨ˆä¿¡æ¯"""
        retry_distribution = {}
        for info in self.failed_components.values():
            retry_count = info['retry_count']
            retry_distribution[retry_count] = retry_distribution.get(retry_count, 0) + 1
            
        return {
            'total_failed': len(self.failed_components),
            'retry_distribution': retry_distribution
        }
    
    def _find_actual_file_path(self, component_id: str, lot_id: str, station: str, 
                               source_product: str, file_type: str) -> Optional[str]:
        """æ™ºèƒ½æŸ¥æ‰¾å¯¦éš›æ–‡ä»¶è·¯å¾‘"""
        try:
            # æå–åŸå§‹ lot_idï¼ˆç§»é™¤ temp_ å‰ç¶´ï¼‰
            original_lot_id = lot_id.replace('temp_', '') if lot_id.startswith('temp_') else lot_id
            
            # æ§‹å»ºæ¨™æº–è·¯å¾‘
            standard_path = config.get_path(
                f"database.structure.{file_type}",
                product=source_product,
                lot=original_lot_id,
                station=station
            )
            
            # æª¢æŸ¥æ¨™æº–è·¯å¾‘æ˜¯å¦å­˜åœ¨
            if Path(standard_path).exists():
                return str(standard_path)
            
            # å¦‚æœæ¨™æº–è·¯å¾‘ä¸å­˜åœ¨ï¼Œå˜—è©¦åœ¨å…¶ä»–ç”¢å“ç›®éŒ„ä¸­æŸ¥æ‰¾
            base_path = Path(config.get("database.base_path", "D:/Database-PC"))
            for product_dir in base_path.iterdir():
                if product_dir.is_dir() and product_dir.name != source_product:
                    test_path = product_dir / original_lot_id / station / file_type
                    if test_path.exists():
                        logger.info(f"åœ¨ç”¢å“ {product_dir.name} ä¸­æ‰¾åˆ° {file_type} æ–‡ä»¶: {test_path}")
                        return str(test_path)
            
            return None
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾æ–‡ä»¶è·¯å¾‘æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def start_scheduler(self, interval_hours: int = 24):
        """å•Ÿå‹•å®šæ™‚å™¨ï¼ˆæ ¹æ“šé…ç½®çš„æ™‚é–“åŸ·è¡Œï¼‰"""
        if self.is_running:
            logger.info("å»¶é²ç§»å‹•èª¿åº¦å™¨å·²åœ¨é‹è¡Œä¸­")
            return
            
        self.is_running = True
        
        # å¾é…ç½®ä¸­è®€å–æ™‚é–“è¨­å®š
        schedule_config = config.get("auto_move.delayed.schedule", {})
        scheduled_time = schedule_config.get("time", "02:00")  # é»˜èªå‡Œæ™¨2é»
        scheduled_days = schedule_config.get("days", ["monday", "tuesday", "wednesday", "thursday", "friday"])
        
        logger.info(f"å»¶é²ç§»å‹•é…ç½® - æ™‚é–“: {scheduled_time}, å¤©æ•¸: {scheduled_days}")
        
        # è§£ææ™‚é–“
        try:
            hour, minute = map(int, scheduled_time.split(":"))
            logger.info(f"è§£ææ™‚é–“æˆåŠŸ - å°æ™‚: {hour}, åˆ†é˜: {minute}")
        except ValueError:
            logger.error(f"ç„¡æ•ˆçš„æ™‚é–“æ ¼å¼: {scheduled_time}ï¼Œä½¿ç”¨é»˜èªæ™‚é–“ 02:00")
            hour, minute = 2, 0
        
        # è¨ˆç®—åˆ°ä¸‹æ¬¡åŸ·è¡Œæ™‚é–“çš„æ¯«ç§’æ•¸
        now = datetime.datetime.now()
        next_run = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
        
        # å¦‚æœä»Šå¤©çš„æ™‚é–“å·²éï¼Œè¨­å®šç‚ºæ˜å¤©
        if next_run <= now:
            next_run += timedelta(days=1)
            logger.info(f"ä»Šå¤©çš„æ™‚é–“å·²éï¼Œè¨­å®šç‚ºæ˜å¤©åŸ·è¡Œ")
        
        delay_ms = (next_run - now).total_seconds() * 1000
        logger.info(f"å»¶é²æ™‚é–“: {delay_ms} æ¯«ç§’ ({delay_ms/1000/60:.1f} åˆ†é˜)")
        
        self.scheduler.start(delay_ms)
        logger.info(f"å»¶é²ç§»å‹•ç®¡ç†å™¨å·²å•Ÿå‹•ï¼Œä¸‹æ¬¡åŸ·è¡Œæ™‚é–“: {next_run} (é…ç½®æ™‚é–“: {scheduled_time})")
    
    def stop_scheduler(self):
        """åœæ­¢å®šæ™‚å™¨"""
        self.is_running = False
        self.scheduler.stop()
        logger.info("å»¶é²ç§»å‹•ç®¡ç†å™¨å·²åœæ­¢")
    
    def process_delayed_moves(self):
        """è™•ç†å»¶é²ç§»å‹•éšŠåˆ—"""
        logger.info("å»¶é²ç§»å‹•ç®¡ç†å™¨é–‹å§‹è™•ç†éšŠåˆ—")
        
        if self.move_queue.empty():
            logger.info("å»¶é²ç§»å‹•éšŠåˆ—ç‚ºç©º")
            # å³ä½¿éšŠåˆ—ç‚ºç©ºï¼Œä¹Ÿè¦é‡æ–°å•Ÿå‹•èª¿åº¦å™¨
            self.start_scheduler()
            return
        
        logger.info("é–‹å§‹è™•ç†å»¶é²ç§»å‹•éšŠåˆ—")
        
        # ç²å–ç›®æ¨™ç”¢å“
        target_product = config.get("auto_move.target_product", "i-Pixel")
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦ç§»å‹•çš„çµ„ä»¶
        components_to_move = []
        queue_size = self.move_queue.qsize()
        logger.info(f"å»¶é²ç§»å‹•éšŠåˆ—ä¸­æœ‰ {queue_size} å€‹é …ç›®")
        
        while not self.move_queue.empty():
            try:
                item = self.move_queue.get_nowait()
                components_to_move.append((
                    item['component_id'],
                    item['lot_id'],
                    item['station'],
                    item['source_product']
                ))
            except Empty:
                break
        
        logger.info(f"æ”¶é›†åˆ° {len(components_to_move)} å€‹çµ„ä»¶éœ€è¦ç§»å‹•")
        
        if components_to_move:
            # ç²å–å»¶é²ç§»å‹•çš„æª”æ¡ˆé¡å‹
            delayed_file_types = config.get("auto_move.delayed.file_types", ["org", "roi"])
            logger.info(f"å»¶é²ç§»å‹•æª”æ¡ˆé¡å‹: {delayed_file_types}")
            
            # å‰µå»ºæ‰¹é‡ç§»å‹•ä»»å‹™
            task_id = data_processor.create_task(
                "batch_move_files",
                target_product,  # ä½¿ç”¨é…ç½®çš„ç›®æ¨™ç”¢å“
                components_to_move[0][1],  # lot_id
                components_to_move[0][2],  # station
                component_id=components_to_move[0][0],  # component_id
                batch_move_params={
                    'components_data': components_to_move,
                    'target_product': target_product,  # ä½¿ç”¨é…ç½®çš„ç›®æ¨™ç”¢å“
                    'file_types': delayed_file_types  # ä½¿ç”¨é…ç½®çš„æª”æ¡ˆé¡å‹
                }
            )
            logger.info(f"å·²å‰µå»ºå»¶é²ç§»å‹•ä»»å‹™: {task_id}")
        else:
            logger.info("æ²’æœ‰çµ„ä»¶éœ€è¦ç§»å‹•")
        
        # é‡æ–°å•Ÿå‹•å®šæ™‚å™¨
        logger.info("é‡æ–°å•Ÿå‹•å»¶é²ç§»å‹•èª¿åº¦å™¨")
        self.is_running = False  # é‡ç½®ç‹€æ…‹
        self.start_scheduler()


class DataProcessor:
    """æ•¸æ“šè™•ç†æ§åˆ¶å™¨ï¼Œæä¾›æ•¸æ“šè™•ç†èˆ‡åœ–åƒç”ŸæˆåŠŸèƒ½"""
    
    _instance = None  # å–®ä¾‹å¯¦ä¾‹
    
    def __new__(cls):
        """å¯¦ç¾å–®ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super(DataProcessor, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–æ•¸æ“šè™•ç†æ§åˆ¶å™¨"""
        if self._initialized:
            return
            
        self._initialized = True
        self.base_path = Path(config.get("database.base_path", "D:/Database-PC"))
        self.active_tasks = {}  # è¿½è¹¤æ´»å‹•ä»»å‹™
        self.task_results = {}  # ä¿å­˜ä»»å‹™çµæœ
        self.task_callbacks = {}  # ä»»å‹™å®Œæˆå›èª¿å‡½æ•¸
        
        # è®€å–è™•ç†é…ç½®
        self.station_order = config.get("processing.station_order", [])
        self.flip_config = config.get("processing.flip_config", {})
        self.station_recipes = config.get("processing.station_recipe", {})
        self.station_logic = config.get("processing.station_logic", {})
        
        # å‰µå»ºä¿¡è™Ÿå™¨å°è±¡ï¼Œç”¨æ–¼ç·šç¨‹å®‰å…¨çš„å›èª¿
        self.signaler = TaskSignaler()
        
        # æ³¨æ„ï¼šå»¶é²ç§»å‹•ç®¡ç†å™¨ç¾åœ¨åœ¨ä¸»è¦–çª—ä¸­åˆå§‹åŒ–ï¼Œä»¥ç¢ºä¿åœ¨æ­£ç¢ºçš„ç·šç¨‹ä¸­é‹è¡Œ
        
        # é‡è©¦éšŠåˆ—å’Œè·¯å¾‘ç›£æ§
        self.retry_queue = {}  # é‡è©¦éšŠåˆ—
        self.path_monitors = {}  # è·¯å¾‘ç›£æ§å™¨
        
        # æ³¨æ„ï¼šå®šæ™‚å™¨éœ€è¦åœ¨Qtä¸»ç·šç¨‹ä¸­å•Ÿå‹•ï¼Œé€™è£¡åªåˆå§‹åŒ–æ•¸æ“šçµæ§‹
        # å¯¦éš›çš„å®šæ™‚å™¨å•Ÿå‹•å°‡åœ¨ä¸»è¦–çª—ä¸­é€²è¡Œ
    
    def _check_path_development_stage(self, base_path: Path, target_path: Path) -> str:
        """æª¢æŸ¥è·¯å¾‘çš„ç™¼å±•éšæ®µ"""
        try:
            if target_path.exists():
                return "complete"  # å®Œæ•´è·¯å¾‘å­˜åœ¨
            elif target_path.parent.exists():
                return "partial"    # éƒ¨åˆ†è·¯å¾‘å­˜åœ¨ï¼ˆæ‰¹æ¬¡æˆ–ç«™é»ç›®éŒ„ï¼‰
            elif base_path.exists():
                return "base"       # åŸºç¤ç›®éŒ„å­˜åœ¨
            else:
                return "none"       # å®Œå…¨ä¸å­˜åœ¨
        except Exception as e:
            logger.error(f"æª¢æŸ¥è·¯å¾‘ç™¼å±•éšæ®µæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return "error"
    
    def _add_to_retry_queue(self, component_id: str, lot_id: str, station: str, 
                           source_product: str, target_product: str, file_types: List[str], 
                           reason: str, retry_delay: int = 300):
        """å°‡çµ„ä»¶æ·»åŠ åˆ°é‡è©¦éšŠåˆ—"""
        try:
            retry_time = datetime.datetime.now() + datetime.timedelta(seconds=retry_delay)
            self.retry_queue[component_id] = {
                'lot_id': lot_id,
                'station': station,
                'source_product': source_product,
                'target_product': target_product,
                'file_types': file_types,
                'reason': reason,
                'retry_time': retry_time,
                'retry_count': 0,
                'max_retries': 5
            }
            logger.info(f"çµ„ä»¶ {component_id} å·²æ·»åŠ åˆ°é‡è©¦éšŠåˆ—ï¼ŒåŸå› : {reason}ï¼Œé‡è©¦æ™‚é–“: {retry_time}")
        except Exception as e:
            logger.error(f"æ·»åŠ çµ„ä»¶ {component_id} åˆ°é‡è©¦éšŠåˆ—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _process_retry_queue(self):
        """è™•ç†é‡è©¦éšŠåˆ—"""
        try:
            current_time = datetime.datetime.now()
            components_to_retry = []
            
            for component_id, retry_info in list(self.retry_queue.items()):
                if current_time >= retry_info['retry_time']:
                    components_to_retry.append((component_id, retry_info))
            
            for component_id, retry_info in components_to_retry:
                if retry_info['retry_count'] < retry_info['max_retries']:
                    logger.info(f"é‡è©¦ç§»å‹•çµ„ä»¶ {component_id} (ç¬¬ {retry_info['retry_count'] + 1} æ¬¡)")
                    self._retry_component_move(component_id, retry_info)
                else:
                    logger.warning(f"çµ„ä»¶ {component_id} å·²è¶…éæœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œå¾é‡è©¦éšŠåˆ—ä¸­ç§»é™¤")
                    del self.retry_queue[component_id]
                    
        except Exception as e:
            logger.error(f"è™•ç†é‡è©¦éšŠåˆ—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _retry_component_move(self, component_id: str, retry_info: dict):
        """é‡è©¦çµ„ä»¶ç§»å‹•"""
        try:
            success, message = self.move_files(
                component_id=component_id,
                lot_id=retry_info['lot_id'],
                station=retry_info['station'],
                source_product=retry_info['source_product'],
                target_product=retry_info['target_product'],
                file_types=retry_info['file_types']
            )
            
            if success:
                logger.info(f"çµ„ä»¶ {component_id} é‡è©¦ç§»å‹•æˆåŠŸ: {message}")
                del self.retry_queue[component_id]
            else:
                # å¢åŠ é‡è©¦æ¬¡æ•¸ï¼Œè¨­ç½®ä¸‹æ¬¡é‡è©¦æ™‚é–“
                retry_info['retry_count'] += 1
                retry_delay = min(300 * (2 ** retry_info['retry_count']), 3600)  # æŒ‡æ•¸é€€é¿ï¼Œæœ€å¤§1å°æ™‚
                retry_info['retry_time'] = datetime.datetime.now() + datetime.timedelta(seconds=retry_delay)
                logger.info(f"çµ„ä»¶ {component_id} é‡è©¦ç§»å‹•å¤±æ•—ï¼Œå°‡åœ¨ {retry_delay} ç§’å¾Œé‡è©¦")
                
        except Exception as e:
            logger.error(f"é‡è©¦çµ„ä»¶ {component_id} ç§»å‹•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _monitor_path_completion(self, component_id: str, lot_id: str, station: str, 
                                source_product: str, target_product: str, file_types: List[str]):
        """ç›£æ§è·¯å¾‘å®Œæˆç‹€æ…‹"""
        try:
            # æª¢æŸ¥æ‰€æœ‰æ–‡ä»¶é¡å‹çš„è·¯å¾‘å®Œæˆç‹€æ…‹
            all_paths_complete = True
            incomplete_paths = []
            
            for file_type in file_types:
                if file_type in ['org', 'roi']:
                    source_path = Path(config.get_path(
                        f"database.structure.{file_type}",
                        product=source_product,
                        lot=lot_id.replace('temp_', '') if lot_id.startswith('temp_') else lot_id,
                        station=station,
                        component=component_id
                    ))
                    
                    if not source_path.exists():
                        all_paths_complete = False
                        incomplete_paths.append(f"{file_type}: {source_path}")
            
            if all_paths_complete:
                logger.info(f"çµ„ä»¶ {component_id} çš„æ‰€æœ‰è·¯å¾‘å·²å®Œæˆï¼Œè‡ªå‹•è§¸ç™¼ç§»å‹•")
                # è‡ªå‹•è§¸ç™¼ç§»å‹•
                success, message = self.move_files(
                    component_id=component_id,
                    lot_id=lot_id,
                    station=station,
                    source_product=source_product,
                    target_product=target_product,
                    file_types=file_types
                )
                
                if success:
                    logger.info(f"çµ„ä»¶ {component_id} è‡ªå‹•ç§»å‹•æˆåŠŸ: {message}")
                    # å¾ç›£æ§åˆ—è¡¨ä¸­ç§»é™¤
                    if component_id in self.path_monitors:
                        del self.path_monitors[component_id]
                else:
                    logger.warning(f"çµ„ä»¶ {component_id} è‡ªå‹•ç§»å‹•å¤±æ•—: {message}")
                    # æ·»åŠ åˆ°é‡è©¦éšŠåˆ—
                    self._add_to_retry_queue(component_id, lot_id, station, source_product, 
                                           target_product, file_types, f"è‡ªå‹•ç§»å‹•å¤±æ•—: {message}")
            else:
                # è·¯å¾‘æœªå®Œæˆï¼Œç¹¼çºŒç›£æ§
                if component_id not in self.path_monitors:
                    self.path_monitors[component_id] = {
                        'lot_id': lot_id,
                        'station': station,
                        'source_product': source_product,
                        'target_product': target_product,
                        'file_types': file_types,
                        'start_time': datetime.datetime.now()
                    }
                    
        except Exception as e:
            logger.error(f"ç›£æ§çµ„ä»¶ {component_id} è·¯å¾‘å®Œæˆç‹€æ…‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _check_path_completion(self):
        """æª¢æŸ¥è·¯å¾‘å®Œæˆç‹€æ…‹"""
        try:
            for component_id, monitor_info in list(self.path_monitors.items()):
                self._monitor_path_completion(
                    component_id=component_id,
                    lot_id=monitor_info['lot_id'],
                    station=monitor_info['station'],
                    source_product=monitor_info['source_product'],
                    target_product=monitor_info['target_product'],
                    file_types=monitor_info['file_types']
                )
        except Exception as e:
            logger.error(f"æª¢æŸ¥è·¯å¾‘å®Œæˆç‹€æ…‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _debug_component_files(self, component_id: str, lot_id: str, station: str, 
                              source_product: str, target_product: str, file_types: List[str]) -> None:
        """èª¿è©¦çµ„ä»¶æª”æ¡ˆç‹€æ…‹ï¼ˆè©³ç´°ç‰ˆæœ¬ï¼Œå¯é…ç½®è¼¸å‡ºè©³ç´°ä¿¡æ¯ï¼‰"""
        try:
            # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨è©³ç´°èª¿è©¦
            enable_detailed_debug = config.get("monitoring.enable_detailed_path_debug", False)
            debug_output = config.get("monitoring.path_debug_output", "terminal")
            
            # ğŸ” ä½¿ç”¨èˆ‡ move_files ä¸€è‡´çš„é‚è¼¯ç²å–åŸå§‹æ‰¹æ¬¡ID
            try:
                lot_obj = db_manager.get_lot(lot_id)
                if lot_obj and hasattr(lot_obj, 'original_lot_id'):
                    original_lot_id = lot_obj.original_lot_id
                else:
                    # å‚™ç”¨æ–¹æ¡ˆï¼šç§»é™¤ temp_ å‰ç¶´
                    original_lot_id = lot_id.replace('temp_', '') if lot_id.startswith('temp_') else lot_id
            except:
                # å‚™ç”¨æ–¹æ¡ˆï¼šç§»é™¤ temp_ å‰ç¶´
                original_lot_id = lot_id.replace('temp_', '') if lot_id.startswith('temp_') else lot_id
            
            if enable_detailed_debug:
                print(f"\nğŸ” è©³ç´°è·¯å¾‘èª¿è©¦ - çµ„ä»¶: {component_id}")
                print(f"   æ‰¹æ¬¡ID: {lot_id} â†’ åŸå§‹æ‰¹æ¬¡ID: {original_lot_id}")
                print(f"   ç«™é»: {station}")
                print(f"   æºç”¢å“: {source_product}")
                print(f"   ç›®æ¨™ç”¢å“: {target_product}")
                print(f"   æ–‡ä»¶é¡å‹: {file_types}")
                print("   " + "="*60)
            
            # ğŸ” æ§‹å»ºæºè·¯å¾‘å’Œç›®æ¨™è·¯å¾‘ï¼ˆèˆ‡ move_files å®Œå…¨ä¸€è‡´ï¼‰
            source_paths = {}
            target_paths = {}
            for file_type in file_types:
                if file_type == 'org':
                    source_paths[file_type] = config.get_path(
                        "database.structure.org",
                        product=source_product,
                        lot=original_lot_id,
                        station=station,
                        component=component_id
                    )
                    target_paths[file_type] = config.get_path(
                        "database.structure.org",
                        product=target_product,
                        lot=original_lot_id,
                        station=station,
                        component=component_id
                    )
                elif file_type == 'roi':
                    source_paths[file_type] = config.get_path(
                        "database.structure.roi",
                        product=source_product,
                        lot=original_lot_id,
                        station=station,
                        component=component_id
                    )
                    target_paths[file_type] = config.get_path(
                        "database.structure.roi",
                        product=target_product,
                        lot=original_lot_id,
                        station=station,
                        component=component_id
                    )
                
                if enable_detailed_debug:
                    print(f"   ğŸ“ {file_type.upper()} è·¯å¾‘ç”Ÿæˆ:")
                    print(f"      é…ç½®æ¨¡æ¿: database.structure.{file_type}")
                    print(f"      åƒæ•¸: product={source_product}, lot={original_lot_id}, station={station}")
                    print(f"      æºè·¯å¾‘: {source_paths[file_type]}")
                    print(f"      ç›®æ¨™è·¯å¾‘: {target_paths[file_type]}")
            
            # ğŸ” æª¢æŸ¥æºè·¯å¾‘å’Œç›®æ¨™è·¯å¾‘ç‹€æ…‹
            for file_type in file_types:
                source_path = source_paths.get(file_type)
                target_path = target_paths.get(file_type)
                
                if source_path and Path(source_path).exists():
                    try:
                        # ä½¿ç”¨ os.listdir é€²è¡Œå¿«é€Ÿæª¢æŸ¥
                        files = os.listdir(source_path)
                        file_count = len(files)
                        
                        if enable_detailed_debug:
                            print(f"   âœ… {file_type.upper()} æºè·¯å¾‘å­˜åœ¨:")
                            print(f"      æºè·¯å¾‘: {source_path}")
                            print(f"      æ–‡ä»¶æ•¸é‡: {file_count}")
                            
                            # é¡¯ç¤ºæ¨£æœ¬æ–‡ä»¶ï¼ˆæœ€å¤š5å€‹ï¼‰
                            if file_count > 0:
                                sample_files = files[:5]
                                print(f"      æ¨£æœ¬æ–‡ä»¶: {sample_files}")
                                if file_count > 5:
                                    print(f"      ... é‚„æœ‰ {file_count - 5} å€‹æ–‡ä»¶")
                            else:
                                print(f"      âš ï¸  ç›®éŒ„ç‚ºç©º")
                        
                        # è¨˜éŒ„åˆ°æ—¥èªŒ
                        if file_count == 0:
                            logger.warning(f"çµ„ä»¶ {component_id} çš„ {file_type} è³‡æ–™å¤¾ç‚ºç©º: {source_path}")
                        
                    except OSError as e:
                        error_msg = f"ç„¡æ³•è®€å– {file_type} è³‡æ–™å¤¾ {source_path}: {e}"
                        if enable_detailed_debug:
                            print(f"   âŒ {file_type.upper()} è®€å–éŒ¯èª¤: {error_msg}")
                        logger.error(error_msg)
                else:
                    error_msg = f"çµ„ä»¶ {component_id} çš„ {file_type} æºè·¯å¾‘ä¸å­˜åœ¨: {source_path}"
                    if enable_detailed_debug:
                        print(f"   âŒ {file_type.upper()} æºè·¯å¾‘ä¸å­˜åœ¨: {error_msg}")
                    logger.warning(error_msg)
                
                # ğŸ” æª¢æŸ¥ç›®æ¨™è·¯å¾‘
                if target_path:
                    if enable_detailed_debug:
                        if Path(target_path).exists():
                            print(f"   âš ï¸  {file_type.upper()} ç›®æ¨™è·¯å¾‘å·²å­˜åœ¨: {target_path}")
                        else:
                            print(f"   ğŸ“ {file_type.upper()} ç›®æ¨™è·¯å¾‘å°‡å‰µå»º: {target_path}")
            
            if enable_detailed_debug:
                print("   " + "="*60)
                print(f"   ğŸ“Š èª¿è©¦å®Œæˆ - çµ„ä»¶: {component_id}\n")
                    
        except Exception as e:
            error_msg = f"èª¿è©¦çµ„ä»¶ {component_id} æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            if config.get("monitoring.enable_detailed_path_debug", False):
                print(f"   ğŸ’¥ èª¿è©¦éŒ¯èª¤: {error_msg}")
            logger.error(error_msg)
    
    def _load_mask_rules(self, station):
        """è¼‰å…¥æŒ‡å®šç«™é»çš„é®ç½©è¦å‰‡"""
        mask_path = Path("config/masks") / f"{station}.json"
        if not mask_path.exists():
            logger.warning(f"æ‰¾ä¸åˆ°ç«™é» {station} çš„é®ç½©è¦å‰‡æª”æ¡ˆ")
            return []
            
        try:
            with open(mask_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥é®ç½©è¦å‰‡å¤±æ•—: {e}")
            return []
    
    def process_csv_header(self, file_path, skiprows=20):
        """è™•ç†CSVæª”æ¡ˆæ¨™é ­ï¼Œåˆªé™¤æ¨™é ­ä¸¦é‡æ–°æ ¼å¼åŒ–"""
        return remove_header_and_rename(file_path)
    
    def generate_basemap(self, component: ComponentInfo) -> Tuple[bool, str]:
        """
        ç”ŸæˆBasemapåœ–åƒ

        æµç¨‹æ ¹æ“šåŸå§‹databasemanager:
        # Step 1: è®€å–configåƒæ•¸(configs/formulasã€masksã€plotsã€sample_rule.jsonç­‰ç­‰)
        # Step 2: åŸå§‹ CSV åç§»ç¢ºèª (rawdata_check.py)
        # Step 3: å»è¡¨é ­ + rename(head_remove.py)
        # Step 4: åš Basemap(basemap_runner.py)
        """
        try:
            if not component.csv_path or not Path(component.csv_path).exists():
                return False, "æ‰¾ä¸åˆ°CSVæª”æ¡ˆ"
            
            csv_filename = Path(component.csv_path).name
            
            # å¦‚æœæ˜¯åŸå§‹æ ¼å¼ {device}_{component}_{time}.csvï¼ŒåŸ·è¡Œ step1~3
            if AOI_FILENAME_PATTERN.match(csv_filename):
                logger.info(f"æª”æ¡ˆ {csv_filename} æ˜¯åŸå§‹æ ¼å¼ï¼ŒåŸ·è¡Œ step1~3")
                
                # å–å¾—ç”¢å“IDï¼Œç”¨æ–¼è¨­å®šæ­£ç¢ºçš„è·¯å¾‘
                product_id = db_manager.get_lot(component.lot_id).product_id
                
                # Step 1: è®€å–configåƒæ•¸
                logger.info(f"Step 1: è®€å–å…ƒä»¶ {component.component_id} çš„configåƒæ•¸")
                sample_rules_path = Path("config/sample_rules.json")
                if not sample_rules_path.exists():
                    return False, "æ‰¾ä¸åˆ°sample_rules.jsoné…ç½®æª”æ¡ˆ"
                with open(sample_rules_path, 'r', encoding='utf-8') as f:
                    sample_rules = json.load(f)
                station = component.station
                rule = sample_rules.get(station, {})
                if not rule:
                    return False, f"åœ¨sample_rulesä¸­æ‰¾ä¸åˆ°ç«™é» {station} çš„é…ç½®"
                formula_path = Path(rule.get("formulas", ""))
                mask_path = Path(rule.get("mask", ""))
                plot_path = Path(rule.get("plot", ""))
                if not formula_path.exists():
                    return False, f"æ‰¾ä¸åˆ°formulaé…ç½®æª”æ¡ˆ: {formula_path}"
                if not mask_path.exists():
                    logger.warning(f"æ‰¾ä¸åˆ°maské…ç½®æª”æ¡ˆ: {mask_path}")
                if not plot_path.exists():
                    return False, f"æ‰¾ä¸åˆ°ploté…ç½®æª”æ¡ˆ: {plot_path}"

                # Step 2: åŸå§‹ CSV åç§»ç¢ºèª
                logger.info(f"Step 2: åŸ·è¡Œ {component.component_id} çš„åŸå§‹CSVåç§»ç¢ºèª")
                align_config_path = Path("config/align_key_config.json")
                if not align_config_path.exists():
                    return False, "æ‰¾ä¸åˆ°align_key_config.jsoné…ç½®æª”æ¡ˆ"
                with open(align_config_path, 'r', encoding='utf-8') as f:
                    align_config = json.load(f)
                recipe = self.station_recipes.get(station, "Sapphire A")
                status, detail = check_csv_alignment(component.csv_path, recipe, align_config)

                if status == "fail":
                    return False, f"CSVæª”æ¡ˆåç§»éŒ¯èª¤: {detail}"

                # Step 3: å»è¡¨é ­ + renameï¼Œä¸¦å­˜å„²åˆ°csvç›®éŒ„
                logger.info(f"Step 3: è™•ç† {component.component_id} çš„CSVæ¨™é ­")
                
                # ç²å–åŸå§‹æ‰¹æ¬¡IDï¼Œç”¨æ–¼æ§‹å»ºè·¯å¾‘
                lot_obj = db_manager.get_lot(component.lot_id)
                product_id = lot_obj.product_id
                original_lot_id = lot_obj.original_lot_id
                
                # è¨­ç½®csvç›®éŒ„è·¯å¾‘ï¼Œä½¿ç”¨åŸå§‹æ‰¹æ¬¡ID
                csv_dir = config.get_path(
                    "database.structure.csv", 
                    product=product_id, 
                    lot=original_lot_id, 
                    station=component.station
                )
                
                # ç¢ºä¿ç›®éŒ„å­˜åœ¨
                ensure_directory(Path(csv_dir))
                
                # æå–component_idç”¨æ–¼é‡å‘½å
                component_id = extract_component_from_filename(Path(component.csv_path).name)
                if not component_id:
                    component_id = component.component_id
                
                # è¨­ç½®è™•ç†å¾Œæ–‡ä»¶çš„ç›®æ¨™è·¯å¾‘
                target_csv_path = os.path.join(csv_dir, f"{component_id}.csv")
                
                # åŸ·è¡Œå»è¡¨é ­å’Œé‡å‘½åï¼Œä¸¦ä¿å­˜åˆ°csvç›®éŒ„
                success, result = remove_header_and_rename(component.csv_path, output_path=target_csv_path)
                if not success:
                    return False, f"è™•ç†CSVæ¨™é ­å¤±æ•—: {result}"
                
                # æ›´æ–°çµ„ä»¶è·¯å¾‘ï¼Œä¿å­˜åŸå§‹è·¯å¾‘
                original_csv_path = component.csv_path
                component.original_csv_path = original_csv_path  # ä¿å­˜åŸå§‹è·¯å¾‘
                component.csv_path = result  # æ›´æ–°ç‚ºè™•ç†å¾Œçš„è·¯å¾‘
                db_manager.update_component(component)
                
                # æ›´æ–°æª”åä»¥ä¾¿å¾ŒçºŒæª¢æŸ¥
                csv_filename = Path(component.csv_path).name
            
            # ç¢ºèªæª”æ¡ˆæ˜¯å¦ç¬¦åˆè™•ç†å¾Œæ ¼å¼ï¼Œå¦‚æœä¸ç¬¦åˆå‰‡å ±éŒ¯
            if not PROCESSED_FILENAME_PATTERN.match(csv_filename):
                return False, f"CSVæª”æ¡ˆ {csv_filename} æ ¼å¼ä¸æ­£ç¢ºï¼Œç„¡æ³•ç”ŸæˆBasemap"

            # Step 4: åš Basemap
            logger.info(f"Step 4: ç‚º {component.component_id} ç”ŸæˆBasemap")
            
            # è®€å– config ç›¸é—œå…§å®¹ï¼ˆå¦‚æœ step1 æœªåŸ·è¡Œï¼‰
            if not 'sample_rules' in locals():
                sample_rules_path = Path("config/sample_rules.json")
                if not sample_rules_path.exists():
                    return False, "æ‰¾ä¸åˆ°sample_rules.jsoné…ç½®æª”æ¡ˆ"
                with open(sample_rules_path, 'r', encoding='utf-8') as f:
                    sample_rules = json.load(f)
                station = component.station
                rule = sample_rules.get(station, {})
                if not rule:
                    return False, f"åœ¨sample_rulesä¸­æ‰¾ä¸åˆ°ç«™é» {station} çš„é…ç½®"
                formula_path = Path(rule.get("formulas", ""))
                mask_path = Path(rule.get("mask", ""))
                plot_path = Path(rule.get("plot", ""))
            
            # è®€å– CSV è³‡æ–™
            df = load_csv(component.csv_path)
            if df is None:
                return False, "è®€å–è™•ç†å¾Œçš„CSVå¤±æ•—"
            
            # æ‡‰ç”¨é®ç½©
            mask_rules = []
            if mask_path.exists():
                try:
                    with open(mask_path, 'r', encoding='utf-8') as f:
                        mask_rules = json.load(f)
                except Exception as e:
                    mask_rules = []
            if mask_rules:
                df = apply_mask(df, mask_rules)
            
            # åŸ·è¡Œç¿»è½‰
            if self.flip_config.get(component.station, False):
                df = flip_data(df)
            
            # è¨­ç½®è¼¸å‡ºè·¯å¾‘ï¼ŒæŒ‰ç…§è¨­å®šæ ¼å¼å­˜å„²
            lot_obj = db_manager.get_lot(component.lot_id)
            product_id = lot_obj.product_id
            original_lot_id = lot_obj.original_lot_id
            
            # ä½¿ç”¨é…ç½®ç²å–æ­£ç¢ºçš„mapç›®éŒ„è·¯å¾‘
            map_dir = config.get_path(
                "database.structure.map",
                product=product_id,
                lot=original_lot_id
            )
            
            # ç¢ºä¿ç«™é»å­ç›®éŒ„å­˜åœ¨
            output_dir = Path(map_dir) / component.station
            ensure_directory(output_dir)
            
            # è¨­ç½®è¼¸å‡ºæ–‡ä»¶å
            component_name = Path(component.csv_path).stem
            output_path = output_dir / f"{component_name}.png"
            
            # è®€å–ç¹ªåœ–é…ç½®
            try:
                with open(plot_path, 'r', encoding='utf-8') as f:
                    plot_config = json.load(f)
            except Exception as e:
                return False, f"è®€å–ç¹ªåœ–é…ç½®å¤±æ•—: {str(e)}"
            
            # ç¹ªè£½ Basemap
            if plot_basemap(df, str(output_path), plot_config=plot_config):
                component.basemap_path = str(output_path)
                db_manager.update_component(component)
                
                # æ–°å¢ï¼šè‡ªå‹•ç§»å‹•å³æ™‚æª”æ¡ˆï¼ˆCSV å’Œ Mapï¼‰
                self._auto_move_immediate_files(component)
                
                logger.info(f"æˆåŠŸç”ŸæˆBasemap: {output_path}")
                return True, str(output_path)
            else:
                return False, "ç”ŸæˆBasemapå¤±æ•—"
                
        except Exception as e:
            logger.error(f"ç”ŸæˆBasemapæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False, f"ç”Ÿæˆå¤±æ•—: {str(e)}"
    
    def generate_lossmap(self, product_id: str, lot_id: str, station: str) -> Tuple[bool, str]:
        """
        ç”ŸæˆæŒ‡å®šç«™é»çš„æ‰€æœ‰å…ƒä»¶çš„Lossmap
        
        Args:
            product_id: ç”¢å“ID
            lot_id: æ‰¹æ¬¡ID
            station: ç«™é»åç¨±
            
        Returns:
            Tuple[bool, str]: (æˆåŠŸç‹€æ…‹, è¨Šæ¯)
        """
        try:
            # ç¢ºèªç«™é»å¯ä»¥ç”ŸæˆLossmap
            if station == self.station_order[0]:
                return False, f"{station} æ˜¯ç¬¬ä¸€ç«™ï¼Œç„¡æ³•ç”ŸæˆLossmap"
                
            # ç²å–å‰ç«™ç´¢å¼•
            station_idx = self.station_order.index(station)
            prev_station = self.station_order[station_idx - 1]
            
            # ç²å–ç«™é»æ‰€æœ‰å…ƒä»¶
            components = db_manager.get_components_by_lot_station(lot_id, station)
            success_count = 0
            fail_count = 0
            
            # ç²å–åŸå§‹æ‰¹æ¬¡IDï¼Œç”¨æ–¼æ§‹å»ºè·¯å¾‘
            lot_obj = db_manager.get_lot(lot_id)
            original_lot_id = lot_obj.original_lot_id
            
            for component in components:
                # ç²å–å°æ‡‰çš„å‰ç«™å…ƒä»¶
                prev_component = db_manager.get_component(lot_id, prev_station, component.component_id)
                if not prev_component:
                    logger.warning(f"æ‰¾ä¸åˆ°å‰ç«™({prev_station})å°æ‡‰çš„å…ƒä»¶: {component.component_id}")
                    fail_count += 1
                    continue
                    
                # æª¢æŸ¥CSVæ˜¯å¦å­˜åœ¨
                if not component.csv_path or not Path(component.csv_path).exists():
                    logger.warning(f"æ‰¾ä¸åˆ°ç•¶å‰ç«™CSV: {component.csv_path}")
                    fail_count += 1
                    continue
                    
                if not prev_component.csv_path or not Path(prev_component.csv_path).exists():
                    logger.warning(f"æ‰¾ä¸åˆ°å‰ç«™CSV: {prev_component.csv_path}")
                    fail_count += 1
                    continue
                    
                # è®€å–ç•¶å‰ç«™èˆ‡å‰ç«™CSV
                df_curr = load_csv(component.csv_path)
                df_prev = load_csv(prev_component.csv_path)
                
                if df_curr is None or df_prev is None:
                    logger.warning(f"è®€å–CSVå¤±æ•—: {component.component_id}")
                    fail_count += 1
                    continue
                
                # è™•ç†ç¿»è½‰
                if self.flip_config.get(station, False):
                    df_curr = flip_data(df_curr)
                if self.flip_config.get(prev_station, False):
                    df_prev = flip_data(df_prev)
                
                # è½‰æ›ç‚ºäºŒé€²åˆ¶æ ¼å¼
                df_curr_bin = convert_to_binary(df_curr)
                df_prev_bin = convert_to_binary(df_prev)
                
                # è¨ˆç®—ç‹€æ…‹é» (åŒ…æ‹¬è‰¯å“â†’è‰¯å“ã€è‰¯å“â†’ç¼ºé™·ã€ç¼ºé™·â†’ç¼ºé™·)
                status_points = calculate_loss_points(df_prev_bin, df_curr_bin)
                
                if status_points.empty:
                    logger.info(f"å…ƒä»¶ç„¡æ•¸æ“šé»: {component.component_id}")
                    success_count += 1
                    continue
                
                # ç¢ºå®šè¼¸å‡ºè·¯å¾‘
                output_dir = config.get_path(
                    "database.structure.map",
                    product=product_id,
                    lot=original_lot_id
                )
                # å»ºç«‹LOSSç«™é»å­ç›®éŒ„
                output_dir = Path(output_dir) / f"LOSS{station_idx}"
                ensure_directory(output_dir)
                output_path = output_dir / f"{component.component_id}.png"
                
                # ç”Ÿæˆåœ–åƒ
                if plot_lossmap(status_points, str(output_path)):
                    # æ›´æ–°å…ƒä»¶è³‡è¨Š
                    component.lossmap_path = str(output_path)
                    db_manager.update_component(component)
                    success_count += 1
                else:
                    fail_count += 1
            
            total_count = success_count + fail_count
            logger.info(f"Lossmapè™•ç†å®Œæˆ: ç¸½è¨ˆ {total_count}, æˆåŠŸ {success_count}, å¤±æ•— {fail_count}")
            
            if fail_count == 0:
                return True, f"æ‰€æœ‰{total_count}å€‹å…ƒä»¶Lossmapå·²ç”Ÿæˆ"
            elif success_count > 0:
                return True, f"éƒ¨åˆ†å®Œæˆ: {success_count}/{total_count}å€‹å…ƒä»¶Lossmapå·²ç”Ÿæˆ"
            else:
                return False, "æ‰€æœ‰å…ƒä»¶Lossmapç”Ÿæˆå¤±æ•—"
                
        except Exception as e:
            logger.error(f"ç”ŸæˆLossmapæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False, f"ç”Ÿæˆå¤±æ•—: {str(e)}"
    
    def generate_fpy(self, product_id: str, lot_id: str, station: str) -> Tuple[bool, str]:
        """
        ç”ŸæˆæŒ‡å®šç«™é»çš„FPYåœ–
        
        Args:
            product_id: ç”¢å“ID
            lot_id: æ‰¹æ¬¡ID
            station: ç«™é»åç¨±
            
        Returns:
            Tuple[bool, str]: (æˆåŠŸç‹€æ…‹, è¨Šæ¯)
        """
        try:
            # ç²å–ç«™é»æ‰€æœ‰å…ƒä»¶
            components = db_manager.get_components_by_lot_station(lot_id, station)
            if not components:
                return False, f"æ‰¾ä¸åˆ°ç«™é» {station} çš„å…ƒä»¶"
            
            # ç²å–ä¹‹å‰æ‰€æœ‰ç«™é» - æå‰è¨ˆç®—ç«™é»ç´¢å¼•ï¼Œé¿å…é‡è¤‡è¨ˆç®—
            try:
                station_idx = self.station_order.index(station)
                prev_stations = self.station_order[:station_idx] if station_idx > 0 else []
            except ValueError:
                logger.error(f"ç«™é» {station} ä¸åœ¨ station_order é…ç½®ä¸­")
                return False, f"ç«™é» {station} æœªåœ¨ç³»çµ±é…ç½®ä¸­å®šç¾©"
            
            # ç²å–åŸå§‹æ‰¹æ¬¡IDï¼Œç”¨æ–¼æ§‹å»ºè·¯å¾‘
            lot_obj = db_manager.get_lot(lot_id)
            original_lot_id = lot_obj.original_lot_id
            
            is_first_station = len(prev_stations) == 0
            if is_first_station:
                logger.warning(f"{station} æ˜¯ç¬¬ä¸€ç«™ï¼Œä½†ä»å°‡ç”ŸæˆFPYåœ–")
                
            # è™•ç†æ¯å€‹å…ƒä»¶
            success_count = 0
            fail_count = 0
            skipped_count = 0
            fpy_summary = []
            
            # é å…ˆç²å–ç¿»è½‰é…ç½®
            current_station_flip = self.flip_config.get(station, False)
            prev_station_flip_config = {ps: self.flip_config.get(ps, False) for ps in prev_stations}
            
            for component in components:
                # æª¢æŸ¥CSVæ˜¯å¦å­˜åœ¨
                if not component.csv_path or not Path(component.csv_path).exists():
                    logger.warning(f"æ‰¾ä¸åˆ°ç•¶å‰ç«™CSV: {component.csv_path}")
                    fail_count += 1
                    continue
                
                # æª¢æŸ¥CSVæª”åæ˜¯å¦ç¬¦åˆè™•ç†å¾Œæ ¼å¼
                csv_filename = Path(component.csv_path).name
                if not PROCESSED_FILENAME_PATTERN.match(csv_filename):
                    logger.warning(f"è·³ééè™•ç†å¾Œæ ¼å¼çš„CSV: {csv_filename}")
                    skipped_count += 1
                    continue
                
                # è®€å–ç•¶å‰ç«™CSV
                df_curr = load_csv(component.csv_path)
                if df_curr is None:
                    logger.warning(f"è®€å–CSVå¤±æ•—: {component.component_id}")
                    fail_count += 1
                    continue
                
                # è™•ç†ç¿»è½‰
                if current_station_flip:
                    df_curr = flip_data(df_curr)
                
                # è½‰æ›ç‚ºäºŒé€²åˆ¶æ ¼å¼
                df_curr_bin = convert_to_binary(df_curr)
                df_curr_bin = df_curr_bin.rename(columns={"binary": f"binary_{station}"})
                
                # æº–å‚™åˆä½µå‰ç«™è³‡æ–™
                all_dfs = [df_curr_bin]
                
                # è™•ç†å‰ç«™è³‡æ–™
                for prev_station in prev_stations:
                    prev_component = db_manager.get_component(lot_id, prev_station, component.component_id)
                    if not prev_component or not prev_component.csv_path or not Path(prev_component.csv_path).exists():
                        logger.warning(f"æ‰¾ä¸åˆ°å‰ç«™({prev_station})å°æ‡‰çš„å…ƒä»¶CSV: {component.component_id}")
                        continue
                    
                    # æª¢æŸ¥å‰ç«™CSVæª”åæ˜¯å¦ç¬¦åˆè™•ç†å¾Œæ ¼å¼
                    prev_csv_filename = Path(prev_component.csv_path).name
                    if not PROCESSED_FILENAME_PATTERN.match(prev_csv_filename):
                        logger.warning(f"è·³éå‰ç«™éè™•ç†å¾Œæ ¼å¼çš„CSV: {prev_csv_filename}")
                        continue
                    
                    df_prev = load_csv(prev_component.csv_path)
                    if df_prev is None:
                        continue
                    
                    # è™•ç†ç¿»è½‰
                    if prev_station_flip_config[prev_station]:
                        df_prev = flip_data(df_prev)
                    
                    # è½‰æ›ç‚ºäºŒé€²åˆ¶æ ¼å¼
                    df_prev_bin = convert_to_binary(df_prev)
                    df_prev_bin = df_prev_bin.rename(columns={"binary": f"binary_{prev_station}"})
                    
                    all_dfs.append(df_prev_bin)
                
                # åˆä½µæ‰€æœ‰ç«™é»è³‡æ–™
                if len(all_dfs) == 1:
                    logger.warning(f"å…ƒä»¶åªæœ‰ç•¶å‰ç«™è³‡æ–™: {component.component_id}")
                    # åƒ…æœ‰ç•¶å‰ç«™è³‡æ–™çš„æƒ…æ³
                    merged_df = all_dfs[0]
                    merged_df["CombinedDefectType"] = merged_df[f"binary_{station}"]
                else:
                    # åˆä½µæ‰€æœ‰ç«™é»è³‡æ–™
                    merged_df = all_dfs[0]
                    for df in all_dfs[1:]:
                        merged_df = pd.merge(merged_df, df, on=["Col", "Row"], how="outer")
                    
                    # å¡«å……ç¼ºå¤±å€¼
                    merged_df = merged_df.fillna(0)
                    
                    # è¨ˆç®—ç¶œåˆç¼ºé™·é¡å‹ (1ä»£è¡¨å…¨éƒ¨ç«™å‡ç‚ºè‰¯å“)
                    binary_cols = [col for col in merged_df.columns if col.startswith("binary_")]
                    merged_df["CombinedDefectType"] = merged_df[binary_cols].min(axis=1)
                
                # è¨ˆç®— FPY æ•¸å€¼
                fpy = merged_df["CombinedDefectType"].mean() * 100
                fpy_summary.append({"ID": component.component_id, "FPY": round(fpy, 2)})
                
                # ç¢ºå®šè¼¸å‡ºè·¯å¾‘
                output_dir = config.get_path(
                    "database.structure.map",
                    product=product_id,
                    lot=original_lot_id
                )
                # å»ºç«‹FPYç«™é»å­ç›®éŒ„
                output_dir = Path(output_dir) / "FPY"
                ensure_directory(output_dir)
                output_path = output_dir / f"{component.component_id}.png"
                
                # ç”Ÿæˆåœ–åƒ
                if plot_fpy_map(merged_df, str(output_path)):
                    # æ›´æ–°å…ƒä»¶è³‡è¨Š
                    component.fpy_path = str(output_path)
                    db_manager.update_component(component)
                    success_count += 1
                else:
                    fail_count += 1
            
            # ç”ŸæˆåŒ¯ç¸½FPYé•·æ¢åœ–
            if fpy_summary:
                summary_df = pd.DataFrame(fpy_summary)
                
                # ä½¿ç”¨é…ç½®çš„è·¯å¾‘ä¿å­˜åŒ¯ç¸½æ•¸æ“š
                output_dir = config.get_path(
                    "database.structure.map",
                    product=product_id,
                    lot=original_lot_id
                )
                fpy_dir = Path(output_dir) / "FPY"
                ensure_directory(fpy_dir)
                
                summary_path = fpy_dir / f"summary_{station}.csv"
                save_df_to_csv(summary_df, str(summary_path))
                
                bar_path = str(summary_path).replace(".csv", ".png")
                plot_fpy_bar(summary_df, bar_path)
            
            total_count = success_count + fail_count
            logger.info(f"FPYè™•ç†å®Œæˆ: ç¸½è¨ˆ {total_count}, æˆåŠŸ {success_count}, å¤±æ•— {fail_count}")
            
            if fail_count == 0:
                return True, f"æ‰€æœ‰{total_count}å€‹å…ƒä»¶FPYå·²ç”Ÿæˆ"
            elif success_count > 0:
                return True, f"éƒ¨åˆ†å®Œæˆ: {success_count}/{total_count}å€‹å…ƒä»¶FPYå·²ç”Ÿæˆ"
            else:
                return False, "æ‰€æœ‰å…ƒä»¶FPYç”Ÿæˆå¤±æ•—"
                
        except Exception as e:
            logger.error(f"ç”ŸæˆFPYæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False, f"ç”Ÿæˆå¤±æ•—: {str(e)}"
    
    def generate_fpy_parallel(self, product_id: str, lot_id: str, station: str) -> Tuple[bool, str]:
        """
        ä½¿ç”¨å¤šç·šç¨‹ä¸¦è¡Œç”ŸæˆæŒ‡å®šç«™é»çš„FPYåœ–
        
        Args:
            product_id: ç”¢å“ID
            lot_id: æ‰¹æ¬¡ID
            station: ç«™é»åç¨±
            
        Returns:
            Tuple[bool, str]: (æˆåŠŸç‹€æ…‹, è¨Šæ¯)
        """
        try:
            # ç²å–ç«™é»æ‰€æœ‰å…ƒä»¶
            components = db_manager.get_components_by_lot_station(lot_id, station)
            if not components:
                return False, f"æ‰¾ä¸åˆ°ç«™é» {station} çš„å…ƒä»¶"
            
            # ç²å–ä¹‹å‰æ‰€æœ‰ç«™é» - æå‰è¨ˆç®—ç«™é»ç´¢å¼•ï¼Œé¿å…é‡è¤‡è¨ˆç®—
            try:
                station_idx = self.station_order.index(station)
                prev_stations = self.station_order[:station_idx] if station_idx > 0 else []
            except ValueError:
                logger.error(f"ç«™é» {station} ä¸åœ¨ station_order é…ç½®ä¸­")
                return False, f"ç«™é» {station} æœªåœ¨ç³»çµ±é…ç½®ä¸­å®šç¾©"
            
            # ç²å–åŸå§‹æ‰¹æ¬¡IDï¼Œç”¨æ–¼æ§‹å»ºè·¯å¾‘
            lot_obj = db_manager.get_lot(lot_id)
            original_lot_id = lot_obj.original_lot_id
            
            is_first_station = len(prev_stations) == 0
            if is_first_station:
                logger.warning(f"{station} æ˜¯ç¬¬ä¸€ç«™ï¼Œä½†ä»å°‡ç”ŸæˆFPYåœ–")
            
            # ç·šç¨‹çµæœè·Ÿè¸ª
            thread_results = []
            success_count = 0
            fail_count = 0
            fpy_summary = []
            summary_lock = threading.Lock()  # ç”¨æ–¼ç·šç¨‹å®‰å…¨åœ°æ›´æ–°å…±äº«æ•¸æ“š
            
            # é å…ˆç²å–ç¿»è½‰é…ç½®
            current_station_flip = self.flip_config.get(station, False)
            prev_station_flip_config = {ps: self.flip_config.get(ps, False) for ps in prev_stations}
            
            # å®šç¾©å–®å€‹å…ƒä»¶è™•ç†å‡½æ•¸
            def process_component(component):
                local_success = False
                local_fpy = None
                
                try:
                    # æª¢æŸ¥CSVæ˜¯å¦å­˜åœ¨
                    if not component.csv_path or not Path(component.csv_path).exists():
                        logger.warning(f"æ‰¾ä¸åˆ°ç•¶å‰ç«™CSV: {component.csv_path}")
                        return False, None
                    
                    # æª¢æŸ¥CSVæª”åæ˜¯å¦ç¬¦åˆè™•ç†å¾Œæ ¼å¼
                    csv_filename = Path(component.csv_path).name
                    if not PROCESSED_FILENAME_PATTERN.match(csv_filename):
                        logger.warning(f"è·³ééè™•ç†å¾Œæ ¼å¼çš„CSV: {csv_filename}")
                        return False, None
                    
                    # è®€å–ç•¶å‰ç«™CSV
                    df_curr = load_csv(component.csv_path)
                    if df_curr is None:
                        logger.warning(f"è®€å–CSVå¤±æ•—: {component.component_id}")
                        return False, None
                    
                    # è™•ç†ç¿»è½‰
                    if current_station_flip:
                        df_curr = flip_data(df_curr)
                    
                    # è½‰æ›ç‚ºäºŒé€²åˆ¶æ ¼å¼
                    df_curr_bin = convert_to_binary(df_curr)
                    df_curr_bin = df_curr_bin.rename(columns={"binary": f"binary_{station}"})
                    
                    # æº–å‚™åˆä½µå‰ç«™è³‡æ–™
                    all_dfs = [df_curr_bin]
                    
                    # è™•ç†å‰ç«™è³‡æ–™
                    for prev_station in prev_stations:
                        prev_component = db_manager.get_component(lot_id, prev_station, component.component_id)
                        if not prev_component or not prev_component.csv_path or not Path(prev_component.csv_path).exists():
                            logger.warning(f"æ‰¾ä¸åˆ°å‰ç«™({prev_station})å°æ‡‰çš„å…ƒä»¶CSV: {component.component_id}")
                            continue
                        
                        # æª¢æŸ¥å‰ç«™CSVæª”åæ˜¯å¦ç¬¦åˆè™•ç†å¾Œæ ¼å¼
                        prev_csv_filename = Path(prev_component.csv_path).name
                        if not PROCESSED_FILENAME_PATTERN.match(prev_csv_filename):
                            logger.warning(f"è·³éå‰ç«™éè™•ç†å¾Œæ ¼å¼çš„CSV: {prev_csv_filename}")
                            continue
                        
                        df_prev = load_csv(prev_component.csv_path)
                        if df_prev is None:
                            continue
                        
                        # è™•ç†ç¿»è½‰
                        if prev_station_flip_config[prev_station]:
                            df_prev = flip_data(df_prev)
                        
                        # è½‰æ›ç‚ºäºŒé€²åˆ¶æ ¼å¼
                        df_prev_bin = convert_to_binary(df_prev)
                        df_prev_bin = df_prev_bin.rename(columns={"binary": f"binary_{prev_station}"})
                        
                        all_dfs.append(df_prev_bin)
                    
                    # åˆä½µæ‰€æœ‰ç«™é»è³‡æ–™
                    if len(all_dfs) == 1:
                        # åƒ…æœ‰ç•¶å‰ç«™è³‡æ–™çš„æƒ…æ³
                        merged_df = all_dfs[0]
                        merged_df["CombinedDefectType"] = merged_df[f"binary_{station}"]
                    else:
                        # åˆä½µæ‰€æœ‰ç«™é»è³‡æ–™
                        merged_df = all_dfs[0]
                        for df in all_dfs[1:]:
                            merged_df = pd.merge(merged_df, df, on=["Col", "Row"], how="outer")
                        
                        # å¡«å……ç¼ºå¤±å€¼
                        merged_df = merged_df.fillna(0)
                        
                        # è¨ˆç®—ç¶œåˆç¼ºé™·é¡å‹ (1ä»£è¡¨å…¨éƒ¨ç«™å‡ç‚ºè‰¯å“)
                        binary_cols = [col for col in merged_df.columns if col.startswith("binary_")]
                        merged_df["CombinedDefectType"] = merged_df[binary_cols].min(axis=1)
                    
                    # è¨ˆç®— FPY æ•¸å€¼
                    fpy = merged_df["CombinedDefectType"].mean() * 100
                    
                    # ç¢ºå®šè¼¸å‡ºè·¯å¾‘
                    output_dir = config.get_path(
                        "database.structure.map",
                        product=product_id,
                        lot=original_lot_id
                    )
                    # å»ºç«‹FPYç«™é»å­ç›®éŒ„
                    output_dir = Path(output_dir) / "FPY"
                    ensure_directory(output_dir)
                    output_path = output_dir / f"{component.component_id}.png"
                    
                    # ç”Ÿæˆåœ–åƒ - ç¢ºä¿ä¸æ¶‰åŠUIæ“ä½œ
                    try:
                        if plot_fpy_map(merged_df, str(output_path)):
                            # å°‡å…ƒä»¶æ›´æ–°æ“ä½œå­˜å„²ç‚ºå›å‚³å€¼ï¼Œé¿å…åœ¨å·¥ä½œç·šç¨‹ä¸­ç›´æ¥æ›´æ–°
                            local_success = True
                            local_fpy = {
                                "ID": component.component_id, 
                                "FPY": round(fpy, 2),
                                "path": str(output_path)  # å°‡è·¯å¾‘å‚³å›ï¼Œè®“ä¸»ç·šç¨‹çµ±ä¸€æ›´æ–°è³‡æ–™åº«
                            }
                    except Exception as img_error:
                        logger.error(f"ç”ŸæˆFPYåœ–åƒå¤±æ•— {component.component_id}: {img_error}")
                        return False, None
                    
                    return local_success, local_fpy
                    
                except Exception as e:
                    logger.error(f"è™•ç†å…ƒä»¶ {component.component_id} FPYæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    return False, None
            
            # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡Œè™•ç†
            max_workers = min(8, len(components))  # æœ€å¤š8å€‹ç·šç¨‹ï¼Œé¿å…éåº¦ä¸¦è¡Œ
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(process_component, comp) for comp in components]
                
                # æ”¶é›†éœ€è¦æ›´æ–°çš„çµ„ä»¶ä¿¡æ¯
                components_to_update = []
                
                for future in futures:
                    success, fpy_data = future.result()
                    with summary_lock:
                        if success:
                            success_count += 1
                            if fpy_data:
                                fpy_summary.append({"ID": fpy_data["ID"], "FPY": fpy_data["FPY"]})
                                # ä¿å­˜éœ€è¦æ›´æ–°çš„çµ„ä»¶è·¯å¾‘ä¿¡æ¯
                                components_to_update.append((fpy_data["ID"], fpy_data["path"]))
                        else:
                            fail_count += 1
            
            # åœ¨ä¸»ç·šç¨‹ä¸­æ›´æ–°çµ„ä»¶ä¿¡æ¯ï¼ˆé¿å…è·¨ç·šç¨‹æ•¸æ“šåº«æ“ä½œï¼‰
            for component_id, fpy_path in components_to_update:
                comp = db_manager.get_component(lot_id, station, component_id)
                if comp:
                    comp.fpy_path = fpy_path
                    db_manager.update_component(comp)
            
            # ç”ŸæˆåŒ¯ç¸½FPYé•·æ¢åœ–
            if fpy_summary:
                summary_df = pd.DataFrame(fpy_summary)
                
                # ä½¿ç”¨é…ç½®çš„è·¯å¾‘ä¿å­˜åŒ¯ç¸½æ•¸æ“š
                output_dir = config.get_path(
                    "database.structure.map",
                    product=product_id,
                    lot=original_lot_id
                )
                fpy_dir = Path(output_dir) / "FPY"
                ensure_directory(fpy_dir)
                
                summary_path = fpy_dir / f"summary_{station}.csv"
                save_df_to_csv(summary_df, str(summary_path))
                
                bar_path = str(summary_path).replace(".csv", ".png")
                plot_fpy_bar(summary_df, bar_path)
            
            total_count = success_count + fail_count
            logger.info(f"FPYä¸¦è¡Œè™•ç†å®Œæˆ: ç¸½è¨ˆ {total_count}, æˆåŠŸ {success_count}, å¤±æ•— {fail_count}")
            
            if fail_count == 0:
                return True, f"æ‰€æœ‰{total_count}å€‹å…ƒä»¶FPYå·²ç”Ÿæˆ"
            elif success_count > 0:
                return True, f"éƒ¨åˆ†å®Œæˆ: {success_count}/{total_count}å€‹å…ƒä»¶FPYå·²ç”Ÿæˆ"
            else:
                return False, "æ‰€æœ‰å…ƒä»¶FPYç”Ÿæˆå¤±æ•—"
                
        except Exception as e:
            logger.error(f"ç”ŸæˆFPYæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False, f"ç”Ÿæˆå¤±æ•—: {str(e)}"
    
    def create_task(self, task_type: str, product_id: str, lot_id: str, station: str = None, 
                   component_id: str = None, callback: Callable = None, **kwargs) -> str:
        """
        å‰µå»ºä¸¦å•Ÿå‹•ä¸€å€‹è™•ç†ä»»å‹™
        
        Args:
            task_type: ä»»å‹™é¡å‹ï¼Œ'basemap', 'lossmap', 'fpy', 'move_files'
            product_id: ç”¢å“ID
            lot_id: æ‰¹æ¬¡ID
            station: ç«™é»åç¨±
            component_id: å…ƒä»¶ID
            callback: ä»»å‹™å®Œæˆå¾Œçš„å›èª¿å‡½æ•¸ï¼ˆå¯é¸ï¼Œå„ªå…ˆä½¿ç”¨ä¿¡è™Ÿæ§½é€£æ¥ï¼‰
            **kwargs: å…¶ä»–ä»»å‹™åƒæ•¸ï¼ˆå¦‚move_filesçš„move_paramsï¼‰
            
        Returns:
            str: ä»»å‹™ID
        """
        task_id = str(uuid.uuid4())
        task = ProcessingTask(
            task_id=task_id,
            task_type=task_type,
            product_id=product_id,
            lot_id=lot_id,
            station=station,
            component_id=component_id
        )
        
        # å¦‚æœæ˜¯ç§»å‹•æª”æ¡ˆä»»å‹™ï¼Œä¿å­˜é¡å¤–çš„åƒæ•¸
        if task_type == "move_files" and "move_params" in kwargs:
            task.move_params = kwargs["move_params"]
        
        # å¦‚æœæ˜¯æ‰¹é‡ç§»å‹•æª”æ¡ˆä»»å‹™ï¼Œä¿å­˜é¡å¤–çš„åƒæ•¸
        if task_type == "batch_move_files" and "batch_move_params" in kwargs:
            task.batch_move_params = kwargs["batch_move_params"]
        
        self.active_tasks[task_id] = task
        
        # å¦‚æœæä¾›äº†callbackï¼Œä¿å­˜å®ƒä½†ä¸å†é€£æ¥ä¿¡è™Ÿ
        # ä¸»è¦–çª—å·²åœ¨åˆå§‹åŒ–æ™‚é€£æ¥äº†ä¿¡è™Ÿæ§½
        if callback:
            self.task_callbacks[task_id] = callback
        
        # å•Ÿå‹•ä»»å‹™åŸ·è¡Œç·’
        thread = threading.Thread(target=self._run_task, args=(task_id,))
        thread.daemon = True
        thread.start()
        
        return task_id
    
    def _run_task(self, task_id: str):
        """
        åœ¨ç¨ç«‹åŸ·è¡Œç·’ä¸­åŸ·è¡Œä»»å‹™
        
        Args:
            task_id: ä»»å‹™ID
        """
        if task_id not in self.active_tasks:
            logger.error(f"æ‰¾ä¸åˆ°ä»»å‹™: {task_id}")
            return
            
        task = self.active_tasks[task_id]
        task.start()
        
        logger.info(f"é–‹å§‹åŸ·è¡Œä»»å‹™: {task_id} ({task.task_type})")
        
        success = False
        message = ""
        
        try:
            # æ€§èƒ½ç›£æ§é–‹å§‹
            import time
            start_time = time.time()
            
            self._monitor_performance(
                f"task_{task.task_type}",
                product_id=task.product_id,
                lot_id=task.lot_id,
                station=task.station,
                task_id=task_id,
                status="é–‹å§‹"
            )
            
            if task.task_type == "process_csv":
                # è™•ç†CSVæ¨™é ­ - å°ˆé–€çš„CSVè™•ç†ä»»å‹™
                if task.component_id:
                    # è™•ç†å–®å€‹å…ƒä»¶çš„CSV
                    component = db_manager.get_component(task.lot_id, task.station, task.component_id)
                    if component and component.csv_path:
                        success, result = self.process_csv_header(component.csv_path)
                        if success:
                            # æ›´æ–°è™•ç†å¾Œçš„CSVè·¯å¾‘
                            component.csv_path = result
                            db_manager.update_component(component)
                            message = f"å·²è™•ç†CSVæ¨™é ­: {result}"
                        else:
                            message = f"è™•ç†CSVæ¨™é ­å¤±æ•—: {result}"
                    else:
                        success, message = False, f"æ‰¾ä¸åˆ°å…ƒä»¶æˆ–CSVè·¯å¾‘: {task.component_id}"
                else:
                    # è™•ç†æ•´å€‹ç«™é»çš„æ‰€æœ‰CSV
                    components = db_manager.get_components_by_lot_station(task.lot_id, task.station)
                    total = len(components)
                    success_count = 0
                    
                    for component in components:
                        if component.csv_path and Path(component.csv_path).exists():
                            result, processed_path = self.process_csv_header(component.csv_path)
                            if result:
                                component.csv_path = processed_path
                                db_manager.update_component(component)
                                success_count += 1
                                
                    success = success_count > 0
                    message = f"å·²è™•ç† {success_count}/{total} å€‹å…ƒä»¶çš„CSVæ¨™é ­"
            
            elif task.task_type == "basemap":
                if task.component_id:
                    # è™•ç†å–®å€‹å…ƒä»¶çš„ basemap
                    component = db_manager.get_component(task.lot_id, task.station, task.component_id)
                    if component:
                        success, message = self.generate_basemap(component)
                    else:
                        success, message = False, f"æ‰¾ä¸åˆ°å…ƒä»¶: {task.component_id}"
                else:
                    # è™•ç†æ•´å€‹ç«™é»çš„ basemap
                    components = db_manager.get_components_by_lot_station(task.lot_id, task.station)
                    total = len(components)
                    success_count = 0
                    
                    for component in components:
                        result, _ = self.generate_basemap(component)
                        if result:
                            success_count += 1
                            
                    success = success_count > 0
                    message = f"å·²è™•ç† {success_count}/{total} å€‹å…ƒä»¶çš„ Basemap"
                    
            elif task.task_type == "lossmap":
                success, message = self.generate_lossmap(task.product_id, task.lot_id, task.station)
                
            elif task.task_type == "fpy":
                success, message = self.generate_fpy(task.product_id, task.lot_id, task.station)
                
            elif task.task_type == "fpy_parallel":
                success, message = self.generate_fpy_parallel(task.product_id, task.lot_id, task.station)
                
            elif task.task_type == "move_files":
                # ç§»å‹•æª”æ¡ˆä»»å‹™
                # å¾taskçš„è‡ªå®šç¾©å±¬æ€§ä¸­ç²å–åƒæ•¸
                if hasattr(task, 'move_params'):
                    params = task.move_params
                    success, message = self.move_files(
                        component_id=task.component_id,
                        lot_id=task.lot_id,
                        station=task.station,
                        source_product=params['source_product'],
                        target_product=params['target_product'],
                        file_types=params['file_types']
                    )
                else:
                    success, message = False, "ç§»å‹•æª”æ¡ˆä»»å‹™ç¼ºå°‘å¿…è¦åƒæ•¸"
                
            elif task.task_type == "batch_move_files":
                # æ‰¹é‡ç§»å‹•æª”æ¡ˆä»»å‹™
                if hasattr(task, 'batch_move_params'):
                    params = task.batch_move_params
                    
                    # ğŸ” è©³ç´°è·¯å¾‘èª¿è©¦ï¼šåœ¨æ‰¹é‡ç§»å‹•å‰æª¢æŸ¥æ¯å€‹çµ„ä»¶çš„å¯¦éš›æ–‡ä»¶çµæ§‹
                    components_data = params['components_data']
                    target_product = params['target_product']
                    file_types = params['file_types']
                    
                    print(f"\nğŸ” å»¶é²ç§»å‹•å‰æª¢æŸ¥ - æ‰¹é‡ç§»å‹• {len(components_data)} å€‹çµ„ä»¶")
                    print(f"   ç›®æ¨™ç”¢å“: {target_product}")
                    print(f"   æ–‡ä»¶é¡å‹: {file_types}")
                    print("   " + "="*60)
                    
                    for index, (component_id, lot_id, station, source_product) in enumerate(components_data):
                        print(f"\nğŸ” å»¶é²ç§»å‹•å‰æª¢æŸ¥ - çµ„ä»¶ {component_id} ({index+1}/{len(components_data)})")
                        self._debug_component_files(
                            component_id=component_id,
                            lot_id=lot_id,
                            station=station,
                            source_product=source_product,
                            target_product=target_product,
                            file_types=file_types
                        )
                    
                    print(f"\nğŸš€ é–‹å§‹åŸ·è¡Œæ‰¹é‡ç§»å‹•...")
                    
                    success, message = self.batch_move_files(
                        components_data=components_data,
                        target_product=target_product,
                        file_types=file_types
                    )
                else:
                    success, message = False, "æ‰¹é‡ç§»å‹•æª”æ¡ˆä»»å‹™ç¼ºå°‘å¿…è¦åƒæ•¸"
                
            else:
                success, message = False, f"æœªçŸ¥çš„ä»»å‹™é¡å‹: {task.task_type}"
                
            # æ€§èƒ½ç›£æ§çµæŸ
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            self._monitor_performance(
                f"task_{task.task_type}",
                product_id=task.product_id,
                lot_id=task.lot_id,
                station=task.station,
                task_id=task_id,
                status="çµæŸ",
                success=success,
                elapsed_time=elapsed_time
            )
            
        except Exception as e:
            logger.error(f"åŸ·è¡Œä»»å‹™ {task_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            success, message = False, f"åŸ·è¡Œå¤±æ•—: {str(e)}"
            
            # è¨˜éŒ„ç•°å¸¸æ€§èƒ½æ•¸æ“š
            try:
                import time
                end_time = time.time()
                elapsed_time = end_time - start_time
                
                self._monitor_performance(
                    f"task_{task.task_type}",
                    product_id=task.product_id,
                    lot_id=task.lot_id,
                    station=task.station,
                    task_id=task_id,
                    status="éŒ¯èª¤",
                    error=str(e),
                    elapsed_time=elapsed_time
                )
            except:
                pass
            
        # æ›´æ–°ä»»å‹™ç‹€æ…‹
        if success:
            task.complete(message)
        else:
            task.fail(message)
            
        # ä¿å­˜ä»»å‹™çµæœ
        self.task_results[task_id] = {
            "success": success,
            "message": message,
            "task": task.to_dict()
        }
        
        # ç™¼å°„ä»»å‹™å®Œæˆä¿¡è™Ÿ - ä½¿ç”¨ä¿¡è™Ÿæ§½æ©Ÿåˆ¶å®‰å…¨åœ°å›èª¿åˆ°UIç·šç¨‹
        try:
            self.signaler.task_completed.emit(task_id, success, message)
        except Exception as e:
            logger.error(f"ç™¼å°„ä»»å‹™å®Œæˆä¿¡è™Ÿæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            # å¦‚æœæœ‰ç›´æ¥å›èª¿ï¼Œä¹Ÿå˜—è©¦èª¿ç”¨å®ƒï¼ˆå‘å¾Œå…¼å®¹ï¼‰
            if task_id in self.task_callbacks:
                try:
                    callback = self.task_callbacks[task_id]
                    # è¨˜éŒ„è­¦å‘Šï¼Œæç¤ºæ‡‰è©²ä½¿ç”¨ä¿¡è™Ÿæ§½æ©Ÿåˆ¶
                    logger.warning(f"ä½¿ç”¨ç›´æ¥å›èª¿å‡½æ•¸è€Œéä¿¡è™Ÿæ§½æ©Ÿåˆ¶: {task_id}")
                    callback(task_id, success, message)
                except Exception as e2:
                    logger.error(f"åŸ·è¡Œç›´æ¥å›èª¿æ™‚ç™¼ç”ŸéŒ¯èª¤: {e2}")
            
        logger.info(f"ä»»å‹™ {task_id} å·²å®Œæˆ: {success} - {message}")
    
    def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """
        ç²å–ä»»å‹™ç‹€æ…‹
        
        Args:
            task_id: ä»»å‹™ID
            
        Returns:
            Dict: ä»»å‹™ç‹€æ…‹
        """
        if task_id in self.task_results:
            return self.task_results[task_id]
            
        if task_id in self.active_tasks:
            task = self.active_tasks[task_id]
            return {
                "success": None,
                "message": f"ä»»å‹™ {task.status}",
                "task": task.to_dict()
            }
            
        return {
            "success": False,
            "message": f"æ‰¾ä¸åˆ°ä»»å‹™: {task_id}",
            "task": None
        }
    
    def cancel_task(self, task_id: str) -> bool:
        """
        å–æ¶ˆæ­£åœ¨åŸ·è¡Œçš„ä»»å‹™ï¼ˆç›®å‰åªèƒ½æ¨™è¨˜ä»»å‹™ç‚ºå¤±æ•—ï¼Œç„¡æ³•çœŸæ­£çµ‚æ­¢ï¼‰
        
        Args:
            task_id: ä»»å‹™ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå–æ¶ˆ
        """
        if task_id in self.active_tasks:
            task = self.active_tasks[task_id]
            if task.status == "running":
                task.fail("ä»»å‹™å·²å–æ¶ˆ")
                self.task_results[task_id] = {
                    "success": False,
                    "message": "ä»»å‹™å·²å–æ¶ˆ",
                    "task": task.to_dict()
                }
                return True
                
        return False
    
    def clean_completed_tasks(self, max_age_hours: int = 24) -> int:
        """
        æ¸…ç†å·²å®Œæˆçš„èˆŠä»»å‹™
        
        Args:
            max_age_hours: ä»»å‹™ä¿ç•™æœ€å¤§å°æ™‚æ•¸
            
        Returns:
            int: æ¸…ç†çš„ä»»å‹™æ•¸é‡
        """
        now = datetime.datetime.now()
        max_age = max_age_hours * 3600  # è½‰æ›ç‚ºç§’
        to_delete = []
        
        for task_id, result in self.task_results.items():
            task = result.get("task")
            if not task or not task.get("end_time"):
                continue
                
            end_time = datetime.datetime.fromisoformat(task["end_time"])
            age = (now - end_time).total_seconds()
            
            if age > max_age:
                to_delete.append(task_id)
                
        # åˆªé™¤èˆŠä»»å‹™
        for task_id in to_delete:
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]
            del self.task_results[task_id]
            
        return len(to_delete)

    def validate_fpy_config(self) -> Dict[str, Any]:
        """
        é©—è­‰FPYç›¸é—œé…ç½®
        
        Returns:
            Dict: é©—è­‰çµæœ
        """
        issues = []
        warnings = []
        
        # æª¢æŸ¥ç«™é»é †åºé…ç½®
        if not self.station_order:
            issues.append("æœªé…ç½®ç«™é»é †åº(station_order)")
        
        # æª¢æŸ¥ç¿»è½‰é…ç½®
        for station in self.station_order:
            if station not in self.flip_config:
                warnings.append(f"ç«™é» {station} ç¼ºå°‘ç¿»è½‰é…ç½®ï¼Œå°‡ä½¿ç”¨é»˜èªå€¼ False")
        
        # æª¢æŸ¥ç«™é»é‚è¼¯é…ç½®
        for station in self.station_order:
            if station not in self.station_logic:
                warnings.append(f"ç«™é» {station} ç¼ºå°‘é‚è¼¯é…ç½®")
            else:
                logic = self.station_logic.get(station, {})
                if "run_fpy" not in logic:
                    warnings.append(f"ç«™é» {station} ç¼ºå°‘ run_fpy é…ç½®ï¼Œå°‡ä½¿ç”¨é»˜èªå€¼ True")
        
        # æª¢æŸ¥è¨ˆç®—ç¬¬ä¸€ç«™FPYçš„åˆç†æ€§
        if self.station_order and self.station_logic:
            first_station = self.station_order[0]
            if first_station in self.station_logic:
                if self.station_logic[first_station].get("run_fpy", False):
                    warnings.append(f"ç¬¬ä¸€ç«™ {first_station} é…ç½®äº†run_fpy=Trueï¼Œä½†ä½œç‚ºç¬¬ä¸€ç«™ç„¡å‰åºç«™é»æ¯”è¼ƒ")
        
        status = len(issues) == 0
        
        if not status:
            logger.error(f"FPYé…ç½®é©—è­‰å¤±æ•—: {issues}")
        
        if warnings:
            logger.warning(f"FPYé…ç½®è­¦å‘Š: {warnings}")
            
        return {
            "status": status,
            "issues": issues,
            "warnings": warnings
        }

    def _monitor_performance(self, func_name, product_id=None, lot_id=None, station=None, **metrics):
        """
        ç›£æ§ä¸¦è¨˜éŒ„æ€§èƒ½æ•¸æ“š
        
        Args:
            func_name: è¢«ç›£æ§çš„å‡½æ•¸åç¨±
            product_id: ç”¢å“ID (å¯é¸)
            lot_id: æ‰¹æ¬¡ID (å¯é¸)
            station: ç«™é»åç¨± (å¯é¸)
            metrics: å…¶ä»–æƒ³è¦è¨˜éŒ„çš„æŒ‡æ¨™
        """
        try:
            import time
            import psutil
            import platform
            
            process = psutil.Process()
            current_memory = process.memory_info().rss / (1024 * 1024)  # MB
            
            perf_data = {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "function": func_name,
                "product_id": product_id,
                "lot_id": lot_id,
                "station": station,
                "memory_usage_mb": current_memory,
                "cpu_percent": psutil.cpu_percent(),
                "thread_count": len(process.threads()),
                "python_version": platform.python_version(),
                **metrics
            }
            
            # è¨˜éŒ„æ€§èƒ½æ•¸æ“š
            log_dir = Path("logs/performance")
            log_dir.mkdir(parents=True, exist_ok=True)
            
            log_file = log_dir / f"perf_{time.strftime('%Y%m%d')}.csv"
            
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œæ±ºå®šæ˜¯å¦éœ€è¦å¯«å…¥è¡¨é ­
            file_exists = log_file.exists()
            
            import csv
            with open(log_file, 'a', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=perf_data.keys())
                if not file_exists:
                    writer.writeheader()
                writer.writerow(perf_data)
                
            logger.debug(f"å·²è¨˜éŒ„æ€§èƒ½æ•¸æ“š: {func_name}, è¨˜æ†¶é«”ä½¿ç”¨: {current_memory:.2f}MB")
            
        except Exception as e:
            logger.error(f"è¨˜éŒ„æ€§èƒ½æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _time_function(self, func, *args, **kwargs):
        """
        æ¸¬é‡å‡½æ•¸åŸ·è¡Œæ™‚é–“çš„è£é£¾å™¨
        
        Args:
            func: è¦æ¸¬é‡çš„å‡½æ•¸
            args, kwargs: å‡½æ•¸åƒæ•¸
            
        Returns:
            å‡½æ•¸çš„è¿”å›å€¼
        """
        import time
        
        func_name = func.__name__
        start_time = time.time()
        
        if 'product_id' in kwargs and 'lot_id' in kwargs and 'station' in kwargs:
            self._monitor_performance(
                func_name, 
                product_id=kwargs['product_id'],
                lot_id=kwargs['lot_id'],
                station=kwargs['station'],
                status="é–‹å§‹"
            )
        
        result = func(*args, **kwargs)
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        if 'product_id' in kwargs and 'lot_id' in kwargs and 'station' in kwargs:
            if isinstance(result, tuple) and len(result) >= 1:
                status = "æˆåŠŸ" if result[0] else "å¤±æ•—"
            else:
                status = "å®Œæˆ"
                
            self._monitor_performance(
                func_name, 
                product_id=kwargs['product_id'],
                lot_id=kwargs['lot_id'],
                station=kwargs['station'],
                elapsed_time=elapsed_time,
                status=status
            )
        
        logger.info(f"å‡½æ•¸ {func_name} åŸ·è¡Œæ™‚é–“: {elapsed_time:.2f}ç§’")
        return result

    def move_files(self, component_id: str, lot_id: str, station: str, 
                   source_product: str, target_product: str, 
                   file_types: List[str]) -> Tuple[bool, str]:
        """
        ç§»å‹•çµ„ä»¶ç›¸é—œæª”æ¡ˆå¾æºç”¢å“åˆ°ç›®æ¨™ç”¢å“
        
        Args:
            component_id: çµ„ä»¶ID
            lot_id: æ‰¹æ¬¡ID
            station: ç«™é»åç¨±
            source_product: æºç”¢å“ID
            target_product: ç›®æ¨™ç”¢å“ID
            file_types: è¦ç§»å‹•çš„æª”æ¡ˆé¡å‹åˆ—è¡¨ ['csv', 'map', 'org', 'roi']
            
        Returns:
            Tuple[bool, str]: (æˆåŠŸç‹€æ…‹, è¨Šæ¯)
        """
        try:
            # ğŸ” è©³ç´°è¨˜éŒ„çµ„ä»¶æŸ¥æ‰¾éç¨‹
            logger.info(f"ğŸ” é–‹å§‹æŸ¥æ‰¾çµ„ä»¶: {component_id}")
            logger.info(f"  æ‰¹æ¬¡ID: {lot_id}")
            logger.info(f"  ç«™é»: {station}")
            logger.info(f"  æºç”¢å“: {source_product}")
            
            # ç²å–çµ„ä»¶ä¿¡æ¯
            component = db_manager.get_component(lot_id, station, component_id)
            if not component:
                logger.warning(f"âŒ é€šé lot_id={lot_id} æ‰¾ä¸åˆ°çµ„ä»¶: {component_id}")
                
                # å˜—è©¦é€šéåŸå§‹æ‰¹æ¬¡IDæŸ¥æ‰¾
                try:
                    lot_obj = db_manager.get_lot(lot_id)
                    if lot_obj and hasattr(lot_obj, 'original_lot_id'):
                        original_lot_id = lot_obj.original_lot_id
                        logger.info(f"ğŸ”„ å˜—è©¦é€šéåŸå§‹æ‰¹æ¬¡IDæŸ¥æ‰¾: {original_lot_id}")
                        component = db_manager.get_component(original_lot_id, station, component_id)
                        if component:
                            logger.info(f"âœ… é€šéåŸå§‹æ‰¹æ¬¡IDæ‰¾åˆ°çµ„ä»¶: {component_id}")
                        else:
                            logger.warning(f"âŒ é€šéåŸå§‹æ‰¹æ¬¡IDä¹Ÿæ‰¾ä¸åˆ°çµ„ä»¶: {component_id}")
                    else:
                        logger.warning(f"âŒ ç„¡æ³•ç²å–æ‰¹æ¬¡ä¿¡æ¯: {lot_id}")
                        return False, f"æ‰¾ä¸åˆ°çµ„ä»¶: {component_id}"
                except Exception as e:
                    logger.error(f"ğŸ’¥ å˜—è©¦åŸå§‹æ‰¹æ¬¡IDæŸ¥æ‰¾æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    return False, f"æ‰¾ä¸åˆ°çµ„ä»¶: {component_id}"
            
            if not component:
                return False, f"æ‰¾ä¸åˆ°çµ„ä»¶: {component_id}"
            
            # ç²å–æ‰¹æ¬¡ä¿¡æ¯ä»¥å–å¾—åŸå§‹æ‰¹æ¬¡ID
            lot_obj = db_manager.get_lot(lot_id)
            if not lot_obj:
                # å¦‚æœç•¶å‰æ‰¹æ¬¡IDæ‰¾ä¸åˆ°ï¼Œå˜—è©¦åŸå§‹æ‰¹æ¬¡ID
                if 'original_lot_id' in locals():
                    lot_obj = db_manager.get_lot(original_lot_id)
                    if not lot_obj:
                        return False, f"æ‰¾ä¸åˆ°æ‰¹æ¬¡: {lot_id} æˆ– {original_lot_id}"
                else:
                    return False, f"æ‰¾ä¸åˆ°æ‰¹æ¬¡: {lot_id}"
            
            original_lot_id = lot_obj.original_lot_id
            
            moved_files = []
            failed_files = []
            
            # è™•ç†æ¯ç¨®æª”æ¡ˆé¡å‹
            for file_type in file_types:
                try:
                    if file_type == 'csv':
                        # CSVæª”æ¡ˆç§»å‹•
                        source_path = config.get_path(
                            "database.structure.csv",
                            product=source_product,
                            lot=original_lot_id,
                            station=station
                        )
                        target_path = config.get_path(
                            "database.structure.csv",
                            product=target_product,
                            lot=original_lot_id,
                            station=station
                        )
                        
                        source_file = Path(source_path) / f"{component_id}.csv"
                        target_file = Path(target_path) / f"{component_id}.csv"
                        
                        if source_file.exists():
                            ensure_directory(target_file.parent)
                            shutil.move(str(source_file), str(target_file))
                            moved_files.append(f"CSV: {source_file} -> {target_file}")
                            
                            # æ›´æ–°çµ„ä»¶çš„CSVè·¯å¾‘
                            component.csv_path = str(target_file)
                        else:
                            failed_files.append(f"CSVæª”æ¡ˆä¸å­˜åœ¨: {source_file}")
                    
                    elif file_type == 'map':
                        # Mapæª”æ¡ˆç§»å‹• (å¯èƒ½åŒ…å«å¤šç¨®é¡å‹çš„map)
                        source_map_base = config.get_path(
                            "database.structure.map",
                            product=source_product,
                            lot=original_lot_id
                        )
                        target_map_base = config.get_path(
                            "database.structure.map",
                            product=target_product,
                            lot=original_lot_id
                        )
                        
                        # æª¢æŸ¥ä¸¦ç§»å‹•å„ç¨®é¡å‹çš„mapæª”æ¡ˆ
                        map_types = [
                            (f"{station}/{component_id}.png", "basemap_path"),
                            (f"LOSS{self.station_order.index(station)}/{component_id}.png", "lossmap_path"),
                            (f"FPY/{component_id}.png", "fpy_path")
                        ]
                        
                        for map_subpath, attr_name in map_types:
                            source_map = Path(source_map_base) / map_subpath
                            target_map = Path(target_map_base) / map_subpath
                            
                            if source_map.exists():
                                ensure_directory(target_map.parent)
                                shutil.move(str(source_map), str(target_map))
                                moved_files.append(f"Map: {source_map} -> {target_map}")
                                
                                # æ›´æ–°çµ„ä»¶çš„mapè·¯å¾‘
                                setattr(component, attr_name, str(target_map))
                    
                    elif file_type == 'org':
                        # Orgè³‡æ–™å¤¾ç§»å‹• - ä½¿ç”¨æ™ºèƒ½è·¯å¾‘æª¢æŸ¥
                        source_org = Path(config.get_path(
                            "database.structure.org",
                            product=source_product,
                            lot=original_lot_id,
                            station=station,
                            component=component_id
                        ))
                        target_org = Path(config.get_path(
                            "database.structure.org",
                            product=target_product,
                            lot=original_lot_id,
                            station=station,
                            component=component_id
                        ))
                        
                        # æª¢æŸ¥åŸºç¤è·¯å¾‘
                        base_org_path = Path(config.get_path(
                            "database.structure.org",
                            product=source_product,
                            lot=original_lot_id,
                            station=station,
                            component=component_id
                        ))
                        
                        # ä½¿ç”¨æ™ºèƒ½è·¯å¾‘æª¢æŸ¥
                        path_stage = self._check_path_development_stage(base_org_path, source_org)
                        
                        # ğŸ” è©³ç´°è¨˜éŒ„æ™ºèƒ½è·¯å¾‘æª¢æŸ¥çµæœ
                        logger.info(f"çµ„ä»¶ {component_id} çš„ ORG è·¯å¾‘æª¢æŸ¥çµæœ: {path_stage}")
                        logger.info(f"  åŸºç¤è·¯å¾‘: {base_org_path}")
                        logger.info(f"  æºè·¯å¾‘: {source_org}")
                        logger.info(f"  ç›®æ¨™è·¯å¾‘: {target_org}")
                        
                        if path_stage == "complete":
                            # è·¯å¾‘å®Œæ•´ï¼ŒåŸ·è¡Œç§»å‹•
                            logger.info(f"çµ„ä»¶ {component_id} çš„ ORG è·¯å¾‘å®Œæ•´ï¼Œé–‹å§‹ç§»å‹•...")
                            try:
                                ensure_directory(target_org.parent)
                                shutil.move(str(source_org), str(target_org))
                                moved_files.append(f"Org: {source_org} -> {target_org}")
                                logger.info(f"âœ… çµ„ä»¶ {component_id} çš„ ORG ç§»å‹•æˆåŠŸ")
                            except Exception as e:
                                error_msg = f"ORGç§»å‹•å¤±æ•—: {str(e)}"
                                failed_files.append(error_msg)
                                logger.error(f"âŒ çµ„ä»¶ {component_id} çš„ {error_msg}")
                        elif path_stage == "partial":
                            # è·¯å¾‘éƒ¨åˆ†å­˜åœ¨ï¼Œæ·»åŠ åˆ°è·¯å¾‘ç›£æ§
                            logger.info(f"ğŸ”„ çµ„ä»¶ {component_id} çš„ ORG è·¯å¾‘éƒ¨åˆ†å­˜åœ¨ï¼Œæ·»åŠ åˆ°è·¯å¾‘ç›£æ§")
                            self._monitor_path_completion(component_id, lot_id, station, source_product, 
                                                       target_product, file_types)
                            failed_files.append(f"ORGè·¯å¾‘éƒ¨åˆ†å­˜åœ¨ï¼Œå·²æ·»åŠ åˆ°è·¯å¾‘ç›£æ§: {source_org}")
                        elif path_stage == "base":
                            # åŸºç¤è·¯å¾‘å­˜åœ¨ï¼Œæ·»åŠ åˆ°è·¯å¾‘ç›£æ§
                            logger.info(f"ğŸ”„ çµ„ä»¶ {component_id} çš„ ORG åŸºç¤è·¯å¾‘å­˜åœ¨ï¼Œæ·»åŠ åˆ°è·¯å¾‘ç›£æ§")
                            self._monitor_path_completion(component_id, lot_id, station, source_product, 
                                                       target_product, file_types)
                            failed_files.append(f"ORGåŸºç¤è·¯å¾‘å­˜åœ¨ï¼Œå·²æ·»åŠ åˆ°è·¯å¾‘ç›£æ§: {source_org}")
                        else:
                            # è·¯å¾‘ä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ°é‡è©¦éšŠåˆ—
                            logger.info(f"â° çµ„ä»¶ {component_id} çš„ ORG è·¯å¾‘ä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ°é‡è©¦éšŠåˆ—")
                            self._add_to_retry_queue(component_id, lot_id, station, source_product, 
                                                   target_product, file_types, f"ORGè·¯å¾‘ä¸å­˜åœ¨: {source_org}")
                            failed_files.append(f"ORGè·¯å¾‘ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ åˆ°é‡è©¦éšŠåˆ—: {source_org}")
                    
                    elif file_type == 'roi':
                        # ROIè³‡æ–™å¤¾ç§»å‹• - ä½¿ç”¨æ™ºèƒ½è·¯å¾‘æª¢æŸ¥
                        source_roi = Path(config.get_path(
                            "database.structure.roi",
                            product=source_product,
                            lot=original_lot_id,
                            station=station,
                            component=component_id
                        ))
                        target_roi = Path(config.get_path(
                            "database.structure.roi",
                            product=target_product,
                            lot=original_lot_id,
                            station=station,
                            component=component_id
                        ))
                        
                        # æª¢æŸ¥åŸºç¤è·¯å¾‘
                        base_roi_path = Path(config.get_path(
                            "database.structure.roi",
                            product=source_product,
                            lot=original_lot_id,
                            station=station,
                            component=component_id
                        ))
                        
                        # ä½¿ç”¨æ™ºèƒ½è·¯å¾‘æª¢æŸ¥
                        path_stage = self._check_path_development_stage(base_roi_path, source_roi)
                        
                        # ğŸ” è©³ç´°è¨˜éŒ„æ™ºèƒ½è·¯å¾‘æª¢æŸ¥çµæœ
                        logger.info(f"çµ„ä»¶ {component_id} çš„ ROI è·¯å¾‘æª¢æŸ¥çµæœ: {path_stage}")
                        logger.info(f"  åŸºç¤è·¯å¾‘: {base_roi_path}")
                        logger.info(f"  æºè·¯å¾‘: {source_roi}")
                        logger.info(f"  ç›®æ¨™è·¯å¾‘: {target_roi}")
                        
                        if path_stage == "complete":
                            # è·¯å¾‘å®Œæ•´ï¼ŒåŸ·è¡Œç§»å‹•
                            logger.info(f"çµ„ä»¶ {component_id} çš„ ROI è·¯å¾‘å®Œæ•´ï¼Œé–‹å§‹ç§»å‹•...")
                            try:
                                ensure_directory(target_roi.parent)
                                shutil.move(str(source_roi), str(target_roi))
                                moved_files.append(f"ROI: {source_roi} -> {target_roi}")
                                logger.info(f"âœ… çµ„ä»¶ {component_id} çš„ ROI ç§»å‹•æˆåŠŸ")
                            except Exception as e:
                                error_msg = f"ROIç§»å‹•å¤±æ•—: {str(e)}"
                                failed_files.append(error_msg)
                                logger.error(f"âŒ çµ„ä»¶ {component_id} çš„ {error_msg}")
                        elif path_stage == "partial":
                            # è·¯å¾‘éƒ¨åˆ†å­˜åœ¨ï¼Œæ·»åŠ åˆ°è·¯å¾‘ç›£æ§
                            logger.info(f"ğŸ”„ çµ„ä»¶ {component_id} çš„ ROI è·¯å¾‘éƒ¨åˆ†å­˜åœ¨ï¼Œæ·»åŠ åˆ°è·¯å¾‘ç›£æ§")
                            self._monitor_path_completion(component_id, lot_id, station, source_product, 
                                                       target_product, file_types)
                            failed_files.append(f"ROIè·¯å¾‘éƒ¨åˆ†å­˜åœ¨ï¼Œå·²æ·»åŠ åˆ°è·¯å¾‘ç›£æ§: {source_roi}")
                        elif path_stage == "base":
                            # åŸºç¤è·¯å¾‘å­˜åœ¨ï¼Œæ·»åŠ åˆ°è·¯å¾‘ç›£æ§
                            logger.info(f"ğŸ”„ çµ„ä»¶ {component_id} çš„ ROI åŸºç¤è·¯å¾‘å­˜åœ¨ï¼Œæ·»åŠ åˆ°è·¯å¾‘ç›£æ§")
                            self._monitor_path_completion(component_id, lot_id, station, source_product, 
                                                       target_product, file_types)
                            failed_files.append(f"ROIåŸºç¤è·¯å¾‘å­˜åœ¨ï¼Œå·²æ·»åŠ åˆ°è·¯å¾‘ç›£æ§: {source_roi}")
                        else:
                            # è·¯å¾‘ä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ°é‡è©¦éšŠåˆ—
                            logger.info(f"â° çµ„ä»¶ {component_id} çš„ ROI è·¯å¾‘ä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ°é‡è©¦éšŠåˆ—")
                            self._add_to_retry_queue(component_id, lot_id, station, source_product, 
                                                   target_product, file_types, f"ROIè·¯å¾‘ä¸å­˜åœ¨: {source_roi}")
                            failed_files.append(f"ROIè·¯å¾‘ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ åˆ°é‡è©¦éšŠåˆ—: {source_roi}")
                            
                except Exception as e:
                    failed_files.append(f"{file_type}ç§»å‹•å¤±æ•—: {str(e)}")
            
            # æ›´æ–°çµ„ä»¶çš„product_id
            component.product_id = target_product
            db_manager.update_component(component)
            
            # æ§‹å»ºçµæœè¨Šæ¯
            success_count = len(moved_files)
            fail_count = len(failed_files)
            
            result_parts = []
            if success_count > 0:
                result_parts.append(f"æˆåŠŸç§»å‹• {success_count} å€‹æª”æ¡ˆ")
            if fail_count > 0:
                result_parts.append(f"å¤±æ•— {fail_count} å€‹æª”æ¡ˆ")
            
            message = "; ".join(result_parts) if result_parts else "æ²’æœ‰æª”æ¡ˆéœ€è¦ç§»å‹•"
            
            # è¨˜éŒ„è©³ç´°ä¿¡æ¯
            if moved_files:
                logger.info(f"æˆåŠŸç§»å‹•çš„æª”æ¡ˆ: {moved_files}")
            if failed_files:
                logger.warning(f"ç§»å‹•å¤±æ•—çš„æª”æ¡ˆ: {failed_files}")
            
            return success_count > 0 or fail_count == 0, message
            
        except Exception as e:
            logger.error(f"ç§»å‹•æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False, f"ç§»å‹•å¤±æ•—: {str(e)}"

    def batch_move_files(self, components_data: List[Tuple[str, str, str, str]], 
                        target_product: str, file_types: List[str]) -> Tuple[bool, str]:
        """
        æ‰¹é‡ç§»å‹•å¤šå€‹çµ„ä»¶çš„æª”æ¡ˆ - å¤šç·šç¨‹ç‰ˆæœ¬
        
        Args:
            components_data: çµ„ä»¶æ•¸æ“šåˆ—è¡¨ [(component_id, lot_id, station, source_product), ...]
            target_product: ç›®æ¨™ç”¢å“ID
            file_types: è¦ç§»å‹•çš„æª”æ¡ˆé¡å‹åˆ—è¡¨ ['csv', 'map', 'org', 'roi']
            
        Returns:
            Tuple[bool, str]: (æˆåŠŸç‹€æ…‹, è¨Šæ¯)
        """
        try:
            from concurrent.futures import ThreadPoolExecutor, as_completed
            from ..controllers.online_monitor import online_manager
            import time
            import threading
            
            total_components = len(components_data)
            success_count = 0
            fail_count = 0
            all_moved_files = []
            all_failed_files = []
            
            logger.info(f"é–‹å§‹æ‰¹é‡ç§»å‹• {total_components} å€‹çµ„ä»¶çš„æª”æ¡ˆ (å¤šç·šç¨‹æ¨¡å¼)")
            
            # å‰µå»ºç¸½é«”é€²åº¦æ—¥èªŒ
            batch_log = online_manager.create_log(
                product_id=target_product,
                lot_id="BATCH",
                station="BATCH_MOVE",
                component_id=f"BATCH_{total_components}_COMPONENTS"
            )
            batch_log.start_processing(f"æ‰¹é‡ç§»å‹• {total_components} å€‹çµ„ä»¶")
            
            def move_single_component(component_data, index):
                """ç§»å‹•å–®å€‹çµ„ä»¶çš„æª”æ¡ˆ"""
                component_id, lot_id, station, source_product = component_data
                thread_id = threading.current_thread().ident
                
                try:
                    # ç‚ºæ¯å€‹çµ„ä»¶å‰µå»ºå–®ç¨çš„æ—¥èªŒ
                    component_log = online_manager.create_log(
                        product_id=target_product,
                        lot_id=lot_id,
                        station=station,
                        component_id=component_id
                    )
                    component_log.start_processing(f"ç§»å‹•æª”æ¡ˆ ({index+1}/{total_components})")
                    
                    # ğŸ” è©³ç´°è·¯å¾‘èª¿è©¦ï¼šåœ¨ç§»å‹•å‰æª¢æŸ¥å¯¦éš›æ–‡ä»¶çµæ§‹
                    if hasattr(self, '_debug_component_files'):
                        logger.info(f"[ç·šç¨‹{thread_id}] ğŸ” å»¶é²ç§»å‹•å‰æª¢æŸ¥ - çµ„ä»¶ {component_id} ({index+1}/{total_components})")
                        self._debug_component_files(
                            component_id=component_id,
                            lot_id=lot_id,
                            station=station,
                            source_product=source_product,
                            target_product=target_product,
                            file_types=file_types
                        )
                    
                    # èª¿ç”¨å–®å€‹æª”æ¡ˆç§»å‹•åŠŸèƒ½
                    logger.info(f"[ç·šç¨‹{thread_id}] ğŸš€ é–‹å§‹ç§»å‹•çµ„ä»¶ {component_id}...")
                    success, message = self.move_files(
                        component_id=component_id,
                        lot_id=lot_id,
                        station=station,
                        source_product=source_product,
                        target_product=target_product,
                        file_types=file_types
                    )
                    
                    if success:
                        logger.info(f"[ç·šç¨‹{thread_id}] âœ… çµ„ä»¶ {component_id} ç§»å‹•æˆåŠŸ: {message}")
                        component_log.complete(f"ç§»å‹•æˆåŠŸ: {message}")
                        online_manager.log_updated.emit(component_log)  # è§¸ç™¼çµ„ä»¶æ—¥èªŒæ›´æ–°
                        return True, f"{component_id}: {message}"
                    else:
                        logger.warning(f"[ç·šç¨‹{thread_id}] âŒ çµ„ä»¶ {component_id} ç§»å‹•å¤±æ•—: {message}")
                        component_log.fail(f"ç§»å‹•å¤±æ•—: {message}")
                        online_manager.log_updated.emit(component_log)  # è§¸ç™¼çµ„ä»¶æ—¥èªŒæ›´æ–°
                        return False, f"{component_id}: {message}"
                        
                except Exception as e:
                    error_msg = f"{component_id}: è™•ç†å¤±æ•— - {str(e)}"
                    logger.error(f"[ç·šç¨‹{thread_id}] ğŸ’¥ ç§»å‹•çµ„ä»¶ {component_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    
                    # æ›´æ–°çµ„ä»¶æ—¥èªŒ
                    if 'component_log' in locals():
                        component_log.fail(f"ç§»å‹•å¤±æ•—: {str(e)}")
                        online_manager.log_updated.emit(component_log)  # è§¸ç™¼çµ„ä»¶æ—¥èªŒæ›´æ–°
                    
                    return False, error_msg
            
            # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡Œè™•ç†ï¼Œé™åˆ¶ä¸¦ç™¼æ•¸é‡é¿å…è³‡æºç«¶çˆ­
            max_workers = min(4, total_components)  # æœ€å¤š4å€‹ä¸¦ç™¼ç·šç¨‹
            processed_count = 0
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»å‹™
                future_to_component = {
                    executor.submit(move_single_component, comp_data, idx): (comp_data, idx)
                    for idx, comp_data in enumerate(components_data)
                }
                
                # è™•ç†å®Œæˆçš„ä»»å‹™
                for future in as_completed(future_to_component):
                    component_data, index = future_to_component[future]
                    component_id = component_data[0]
                    processed_count += 1
                    
                    try:
                        success, message = future.result()
                        
                        if success:
                            success_count += 1
                            all_moved_files.append(message)
                        else:
                            fail_count += 1
                            all_failed_files.append(message)
                        
                        # æ›´æ–°æ‰¹æ¬¡é€²åº¦
                        progress_msg = f"è™•ç†é€²åº¦: {processed_count}/{total_components} (æˆåŠŸ: {success_count}, å¤±æ•—: {fail_count})"
                        batch_log.update_status("processing", progress_msg)
                        online_manager.log_updated.emit(batch_log)  # æ‰‹å‹•è§¸ç™¼æ›´æ–°ä¿¡è™Ÿ
                        logger.info(f"ğŸ“Š æ‰¹é‡ç§»å‹•é€²åº¦: {progress_msg}")
                        
                        # è¨˜éŒ„è©³ç´°çš„æˆåŠŸ/å¤±æ•—ä¿¡æ¯
                        if success:
                            logger.info(f"âœ… çµ„ä»¶ {component_id} è™•ç†å®Œæˆ: {message}")
                        else:
                            logger.warning(f"âŒ çµ„ä»¶ {component_id} è™•ç†å¤±æ•—: {message}")
                        
                    except Exception as e:
                        fail_count += 1
                        error_msg = f"{component_id}: åŸ·è¡Œç•°å¸¸ - {str(e)}"
                        all_failed_files.append(error_msg)
                        logger.error(f"è™•ç†çµ„ä»¶ {component_id} çš„Futureæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        
                        # æ›´æ–°æ‰¹æ¬¡é€²åº¦ (å³ä½¿å‡ºéŒ¯ä¹Ÿè¦æ›´æ–°)
                        progress_msg = f"è™•ç†é€²åº¦: {processed_count}/{total_components} (æˆåŠŸ: {success_count}, å¤±æ•—: {fail_count})"
                        batch_log.update_status("processing", progress_msg)
                        online_manager.log_updated.emit(batch_log)  # æ‰‹å‹•è§¸ç™¼æ›´æ–°ä¿¡è™Ÿ
                        logger.warning(f"ğŸ’¥ çµ„ä»¶ {component_id} åŸ·è¡Œç•°å¸¸: {str(e)}")
            
            # æ§‹å»ºçµæœè¨Šæ¯
            result_parts = []
            if success_count > 0:
                result_parts.append(f"æˆåŠŸè™•ç† {success_count} å€‹çµ„ä»¶")
            if fail_count > 0:
                result_parts.append(f"å¤±æ•— {fail_count} å€‹çµ„ä»¶")
            
            message = "; ".join(result_parts) if result_parts else "æ²’æœ‰çµ„ä»¶éœ€è¦è™•ç†"
            
            # è¨˜éŒ„è©³ç´°ä¿¡æ¯
            if all_moved_files:
                logger.info(f"æ‰¹é‡ç§»å‹•æˆåŠŸçš„çµ„ä»¶: {all_moved_files}")
            if all_failed_files:
                logger.warning(f"æ‰¹é‡ç§»å‹•å¤±æ•—çš„çµ„ä»¶: {all_failed_files}")
            
            overall_success = success_count > 0 or fail_count == 0
            
            # å®Œæˆæ‰¹æ¬¡æ—¥èªŒ
            if overall_success:
                batch_log.complete(f"æ‰¹é‡ç§»å‹•å®Œæˆ: {message}")
            else:
                batch_log.fail(f"æ‰¹é‡ç§»å‹•å¤±æ•—: {message}")
            
            # æ‰‹å‹•è§¸ç™¼æœ€çµ‚æ›´æ–°ä¿¡è™Ÿ
            online_manager.log_updated.emit(batch_log)
            
            return overall_success, message
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ç§»å‹•æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            # å¦‚æœæ‰¹æ¬¡æ—¥èªŒå·²å‰µå»ºï¼Œæ›´æ–°å…¶ç‹€æ…‹
            if 'batch_log' in locals():
                batch_log.fail(f"æ‰¹é‡ç§»å‹•ç•°å¸¸: {str(e)}")
            
            return False, f"æ‰¹é‡ç§»å‹•å¤±æ•—: {str(e)}"

    def _auto_move_immediate_files(self, component: ComponentInfo):
        """
        è‡ªå‹•ç§»å‹•å³æ™‚ç”Ÿæˆçš„æª”æ¡ˆï¼ˆCSV å’Œ Mapï¼‰
        
        Args:
            component: çµ„ä»¶è³‡è¨Š
        """
        try:
            # æª¢æŸ¥è‡ªå‹•ç§»å‹•æ˜¯å¦å•Ÿç”¨
            if not config.get("auto_move.enabled", False):
                logger.info("è‡ªå‹•ç§»å‹•åŠŸèƒ½å·²ç¦ç”¨")
                return
            
            # ç²å–ç›®æ¨™ç”¢å“
            target_product = config.get("auto_move.target_product", "i-Pixel")
            
            # ç²å–æ‰¹æ¬¡ä¿¡æ¯ä»¥å–å¾—ç”¢å“ID
            lot_obj = db_manager.get_lot(component.lot_id)
            if not lot_obj:
                logger.warning(f"æ‰¾ä¸åˆ°æ‰¹æ¬¡: {component.lot_id}ï¼Œç„¡æ³•ç§»å‹•å³æ™‚æª”æ¡ˆ")
                return
            
            source_product = lot_obj.product_id
            
            # å¦‚æœçµ„ä»¶å·²ç¶“åœ¨ç›®æ¨™ç”¢å“ä¸­ï¼Œä¸éœ€è¦ç§»å‹•
            if source_product == target_product:
                logger.info(f"çµ„ä»¶ {component.component_id} å·²åœ¨ç›®æ¨™ç”¢å“ {target_product} ä¸­")
                return
            
            original_lot_id = lot_obj.original_lot_id
            station = component.station
            component_id = component.component_id
            
            # ç²å–å³æ™‚æª”æ¡ˆé¡å‹
            immediate_file_types = config.get("auto_move.immediate.file_types", ["csv", "map"])
            
            # åŸ·è¡Œç§»å‹•
            success, message = self.move_files(
                component_id=component_id,
                lot_id=component.lot_id,
                station=station,
                source_product=source_product,
                target_product=target_product,
                file_types=immediate_file_types
            )
            
            if success:
                logger.info(f"è‡ªå‹•ç§»å‹•å³æ™‚æª”æ¡ˆæˆåŠŸ: {component_id} -> {target_product}")
                
                # æ·»åŠ åˆ°å»¶é²ç§»å‹•éšŠåˆ—ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                if config.get("auto_move.delayed.enabled", False):
                    # ä½¿ç”¨å…¨å±€å»¶é²ç§»å‹•ç®¡ç†å™¨
                    delayed_manager = get_global_delayed_move_manager()
                    if delayed_manager:
                        delayed_manager.add_to_delayed_queue(
                            component_id, component.lot_id, station, 
                            source_product, target_product
                        )
                        logger.info(f"å·²æ·»åŠ åˆ°å»¶é²ç§»å‹•éšŠåˆ—: {component_id}")
                    else:
                        logger.warning("å…¨å±€å»¶é²ç§»å‹•ç®¡ç†å™¨æœªåˆå§‹åŒ–")
            else:
                logger.error(f"è‡ªå‹•ç§»å‹•å³æ™‚æª”æ¡ˆå¤±æ•—: {message}")
                
        except Exception as e:
            logger.error(f"è‡ªå‹•ç§»å‹•å³æ™‚æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def _move_file_or_folder(self, source_path: str, target_path: str, attr_name: str):
        """
        ç§»å‹•å–®å€‹æª”æ¡ˆæˆ–è³‡æ–™å¤¾
        
        Args:
            source_path: æºè·¯å¾‘
            target_path: ç›®æ¨™è·¯å¾‘
            attr_name: çµ„ä»¶å±¬æ€§åç¨± (e.g., 'basemap_path', 'lossmap_path', 'fpy_path', 'csv_path')
        """
        try:
            # ç¢ºä¿ç›®æ¨™ç›®éŒ„å­˜åœ¨
            ensure_directory(target_path)
            
            # æª¢æŸ¥æºæª”æ¡ˆæ˜¯å¦å­˜åœ¨
            if Path(source_path).exists():
                # æª¢æŸ¥æºæª”æ¡ˆæ˜¯å¦ç‚ºè³‡æ–™å¤¾
                if Path(source_path).is_dir():
                    shutil.move(str(source_path), str(target_path))
                else:
                    # æª”æ¡ˆç§»å‹•
                    shutil.move(str(source_path), str(target_path))
            else:
                logger.warning(f"æºæª”æ¡ˆä¸å­˜åœ¨ï¼Œç„¡æ³•ç§»å‹•: {source_path}")
                
        except Exception as e:
            logger.error(f"ç§»å‹•æª”æ¡ˆæˆ–è³‡æ–™å¤¾æ™‚ç™¼ç”ŸéŒ¯èª¤ ({attr_name}): {e}")


# å‰µå»ºå…¨å±€æ•¸æ“šè™•ç†å™¨å¯¦ä¾‹
data_processor = DataProcessor() 

# å…¨å±€å»¶é²ç§»å‹•ç®¡ç†å™¨å¯¦ä¾‹ç®¡ç†
_global_delayed_move_manager = None

def get_global_delayed_move_manager() -> Optional['DelayedMoveManager']:
    """ç²å–å…¨å±€å»¶é²ç§»å‹•ç®¡ç†å™¨å¯¦ä¾‹"""
    return _global_delayed_move_manager

def set_global_delayed_move_manager(manager: 'DelayedMoveManager'):
    """è¨­ç½®å…¨å±€å»¶é²ç§»å‹•ç®¡ç†å™¨å¯¦ä¾‹"""
    global _global_delayed_move_manager
    _global_delayed_move_manager = manager
    logger.info("å…¨å±€å»¶é²ç§»å‹•ç®¡ç†å™¨å¯¦ä¾‹å·²è¨­ç½®")